{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136545db",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "This notebook contains the preprocessing steps for the UN resolutions dataset. The goal is to create a clean and structured dataset that can be used for analysis and modeling.\n",
    "\n",
    "The current pipeline roughly looks like this:\n",
    "\n",
    "1. **Fetch Resolution**: Download the raw UN resolutions data from the source.\n",
    "2. **Transform Resolutions**: Change structure to one row per resolution.\n",
    "3. **Parse Subjects**: Subjects are currently a single string, that may contain multiple subjects. We create one row per resolution-subject pair.\n",
    "4. **Fetch Thesaurus**: Download the thesaurus data from the source (TTL file)\n",
    "5. **Parse Thesaurus Graph**: Parse the TTL file to extract the hierarchical relationships between subjects (using SKOS broader/narrower relationships).\n",
    "6. **Create Subject Lookup Table**: Build a subject reference table with subject_id (URI), labels in different languages, and any other metadata from the thesaurus.\n",
    "7. **Transform Subjects**: Change subject string to subject_id, which allows for multiple languages.\n",
    "8. **Normalize Dataframe**: Create separate tables for resolutions and subjects, and a mapping table for resolution-subject pairs.\n",
    "9. **Build Hierarchy Graph**: Create a directed graph structure from the thesaurus where edges represent parent-child relationships.\n",
    "10. **Generate Closure Table**: Create a closure table that contains all ancestor-descendant pairs with their depths. This includes:\n",
    "    - Self-references (each subject to itself at depth 0)\n",
    "    - All transitive relationships (every ancestor-descendant pair with their distance)\n",
    "11. **Index Tables**: Create indexes on foreign keys and frequently queried columns (resolution_id, subject_id, ancestor_id, descendant_id) for performance.\n",
    "12. **Implement Filter Functions**: Create query functions that use the closure table to efficiently filter resolutions by any category level (including all descendants)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422cae45",
   "metadata": {},
   "source": [
    "Some subjects are in different schemes! example-> 1002319 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f7dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resolution_analyzer import UNResolutionAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbb2a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Logging setup complete.\n",
      "INFO - Initializing UNResolutionAnalyzer\n",
      "INFO - Local data not found or incomplete, fetching and processing data.\n",
      "INFO - Initializing UNResolutionAnalyzer\n",
      "INFO - Local data not found or incomplete, fetching and processing data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janic\\OneDrive\\Desktop\\ETH\\UN Projekt\\policy-pulse\\notebooks\\janic\\resolution_analyzer.py:188: DtypeWarning: Columns (5,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ga = pd.read_csv(ga_url)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Expanded subjects from 443 to 255 rows.\n",
      "INFO - Downloaded thesaurus file successfully\n",
      "INFO - Extracted 7341 subjects/schemes:\n",
      "INFO -   - Concepts: 7322\n",
      "INFO -   - Schemes: 19\n",
      "INFO - Mapping Results:\n",
      "INFO -   Total rows: 5856\n",
      "INFO -   Matched: 3390 (57.9%)\n",
      "INFO -   Unmatched: 2466 (42.1%)\n",
      "INFO - \n",
      "Sample unmatched subjects:\n",
      "INFO -   - 'nan'\n",
      "INFO -   - 'UN. ECONOMIC AND SOCIAL COUNCIL'\n",
      "INFO -   - 'REVIEW CONFERENCE OF THE PARTIES TO THE TREATY ON THE NON-PROLIFERATION OF NUCLEAR WEAPONS (3RD : 1985 : GENEVA)'\n",
      "INFO -   - 'NUCLEAR WEAPON FREEZE'\n",
      "INFO -   - 'NEW INTERNATIONAL ECONOMIC ORDER'\n",
      "INFO -   - 'PEACE AND SECURITY'\n",
      "INFO -   - 'UNITAR'\n",
      "INFO -   - 'UN'\n",
      "INFO -   - 'LEAGUE OF ARAB STATES'\n",
      "INFO -   - 'RIGHT OF PEOPLES TO PEACE'\n",
      "INFO - Normalization Results:\n",
      "INFO - \n",
      "Resolutions Table:\n",
      "INFO -   Total resolutions: 5534\n",
      "INFO -   Columns: undl_id, date, session, resolution, draft, committee_report, meeting, title, agenda_title, total_yes, total_no, total_abstentions, total_non_voting, total_ms, undl_link, AFG, AGO, ALB, AND, ARE, ARG, ARM, ATG, AUS, AUT, AZE, BDI, BEL, BEN, BFA, BGD, BGR, BHR, BHS, BIH, BLR, BLZ, BOL, BRA, BRB, BRN, BTN, BWA, CAF, CAN, CHE, CHL, CHN, CIV, CMR, COD, COG, COL, COM, CPV, CRI, CSK, CUB, CYP, CZE, DDR, DEU, DJI, DMA, DNK, DOM, DZA, EAT, EAZ, ECU, EGY, ERI, ESP, EST, ETH, FIN, FJI, FRA, FSM, GAB, GBR, GEO, GER, GHA, GIN, GMB, GNB, GNQ, GRC, GRD, GTM, GUY, HND, HRV, HTI, HUN, IDN, IND, IRL, IRN, IRQ, ISL, ISR, ITA, JAM, JOR, JPN, KAZ, KEN, KGZ, KHM, KIR, KNA, KOR, KWT, LAO, LBN, LBR, LBY, LCA, LIE, LKA, LSO, LTU, LUX, LVA, MAR, MCO, MDA, MDG, MDV, MEX, MHL, MKD, MLI, MLT, MMR, MNE, MNG, MOZ, MRT, MUS, MWI, MYS, NAM, NER, NGA, NIC, NLD, NOR, NPL, NRU, NZL, OMN, PAK, PAN, PER, PHL, PLW, PNG, POL, PRK, PRT, PRY, QAT, ROU, RUS, RWA, SAU, SCG, SDN, SEN, SGP, SLB, SLE, SLV, SMR, SOM, SRB, SSD, STP, SUN, SUR, SVK, SVN, SWE, SWZ, SYC, SYR, TCD, TGO, THA, TJK, TKM, TLS, TON, TTO, TUN, TUR, TUV, TZA, UGA, UKR, URY, USA, UZB, VCT, VEN, VNM, VUT, WSM, YEM, YMD, YUG, ZAF, ZMB, ZWE\n",
      "INFO - \n",
      "Resolution-Subject Mapping Table:\n",
      "INFO -   Total mappings: 3299\n",
      "INFO -   Unique resolutions with subjects: 3102\n",
      "INFO -   Unique subjects used: 182\n",
      "INFO -   Avg subjects per resolution: 1.06\n",
      "INFO - \n",
      "Warning: 2432 resolutions have no mapped subjects\n",
      "INFO - Found 7341 subjects/schemes\n",
      "INFO - Found 12776 parent-child relationships\n",
      "INFO - \n",
      "Closure table statistics:\n",
      "INFO -   Total rows: 39,026\n",
      "INFO -   Nodes with ancestors: 7,322\n",
      "INFO -   Unique ancestors: 7,341\n",
      "INFO -   Unique descendants: 7,341\n",
      "INFO -   Max depth: 7\n",
      "INFO - \n",
      "Depth distribution:\n",
      "INFO -   Depth 0: 7,341 relationships\n",
      "INFO -   Depth 1: 12,776 relationships\n",
      "INFO -   Depth 2: 12,609 relationships\n",
      "INFO -   Depth 3: 3,878 relationships\n",
      "INFO -   Depth 4: 1,787 relationships\n",
      "INFO -   Depth 5: 552 relationships\n",
      "INFO -   Depth 6: 75 relationships\n",
      "INFO -   Depth 7: 8 relationships\n"
     ]
    }
   ],
   "source": [
    "# Examples of how to use the UNResolutionAnalyzer class\n",
    "\n",
    "# 1. Basic initialization with default configuration\n",
    "analyzer = UNResolutionAnalyzer(config_path='config/data_sources2.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b21e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "Final result: 5534 resolutions\n",
      "Total resolutions: 5534\n"
     ]
    }
   ],
   "source": [
    "# 2. Query all resolutions (no filters)\n",
    "all_resolutions = analyzer.query()\n",
    "print(f\"Total resolutions: {len(all_resolutions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23386bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "Final result: 835 resolutions\n",
      "Resolutions from 2000-2010: 835\n"
     ]
    }
   ],
   "source": [
    "# 3. Query by date range\n",
    "date_filtered = analyzer.query(start_date='2000-01-01', end_date='2010-12-31')\n",
    "print(f\"Resolutions from 2000-2010: {len(date_filtered)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cedb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Expanded 1 subjects to 1157 (including descendants)\n",
      "INFO - After subject filter: 1872 resolutions\n",
      "INFO - \n",
      "Final result: 1872 resolutions\n",
      "Political and Legal Questions related resolutions: 1872\n"
     ]
    }
   ],
   "source": [
    "# 4. Query by subject with descendants\n",
    "# Using 'Political and Legal Questions' subject\n",
    "political_legal_questions_resolutions = analyzer.query(\n",
    "    subject_ids=['http://metadata.un.org/thesaurus/01'],\n",
    "    include_descendants=True\n",
    ")\n",
    "print(f\"Political and Legal Questions related resolutions: {len(political_legal_questions_resolutions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ff422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - After subject filter: 172 resolutions\n",
      "INFO - \n",
      "Final result: 172 resolutions\n",
      "Palestine Questions (strict) resolutions: 172\n"
     ]
    }
   ],
   "source": [
    "# 5. Query by subject without descendants\n",
    "palestine_questions_resolutions = analyzer.query(\n",
    "    subject_ids=['http://metadata.un.org/thesaurus/1004700'],\n",
    "    include_descendants=False\n",
    ")\n",
    "print(f\"Palestine Questions (strict) resolutions: {len(palestine_questions_resolutions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279fd1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Expanded 1 subjects to 723 (including descendants)\n",
      "INFO - After subject filter: 33 resolutions\n",
      "INFO - \n",
      "Final result: 33 resolutions\n",
      "Recent science and technology resolutions: 33\n"
     ]
    }
   ],
   "source": [
    "# 6. Combined query (date range and subject)\n",
    "science_technology_recent = analyzer.query(\n",
    "    start_date='2015-01-01',\n",
    "    subject_ids=['http://metadata.un.org/thesaurus/16'],  # Science and technology\n",
    "    include_descendants=True\n",
    ")\n",
    "print(f\"Recent science and technology resolutions: {len(science_technology_recent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e6f3b",
   "metadata": {},
   "source": [
    "# Tests of new more abstracted pipeline with better separation of concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c692df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unDataStream.py\n",
    "import logging\n",
    "import sys\n",
    "import yaml\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Protocol\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import requests\n",
    "from rdflib import Graph, RDF, SKOS\n",
    "\n",
    "# Abstract base classes for extensibility\n",
    "class DatasetFetcher(ABC):\n",
    "    \"\"\"Abstract base class for dataset-specific fetchers.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fetch(self, source_config: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"Fetch raw data for this dataset type.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_dataset_type(self) -> str:\n",
    "        \"\"\"Return the dataset type identifier (e.g., 'ga_resolutions', 'sc_resolutions').\"\"\"\n",
    "        pass\n",
    "\n",
    "class DatasetProcessor(ABC):\n",
    "    \"\"\"Abstract base class for dataset-specific processors.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, raw_data: pd.DataFrame, **kwargs) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Process raw data into normalized tables.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod \n",
    "    def get_dataset_type(self) -> str:\n",
    "        \"\"\"Return the dataset type this processor handles.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Implementations for GA resolutions\n",
    "class GAResolutionFetcher(DatasetFetcher):\n",
    "    \"\"\"Fetches General Assembly resolution data.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def fetch(self, source_config: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"Fetch GA resolution data from URL.\"\"\"\n",
    "        self.logger.info(f\"Fetching GA resolutions from {source_config['url']}\")\n",
    "        try:\n",
    "            df = pd.read_csv(source_config['url'])\n",
    "            self.logger.info(f\"Successfully fetched {len(df)} GA resolution records\")\n",
    "            \n",
    "            df['session'] = df['session'].astype(str) # Ensure session is consistent\n",
    "            df['date'] = pd.to_datetime(df['date']) # Convert date to datetime\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to fetch GA resolutions: {e}\")\n",
    "            raise\n",
    "    \n",
    "        \n",
    "    def get_dataset_type(self) -> str:\n",
    "        return \"ga_resolutions\"\n",
    "\n",
    "class GAResolutionProcessor(DatasetProcessor):\n",
    "    \"\"\"Processes General Assembly resolution data.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def process(self, raw_data: pd.DataFrame, **kwargs) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Transform GA data from country-per-row to resolution-per-row format.\"\"\"\n",
    "        self.logger.info(\"Processing GA resolution data\")\n",
    "        \n",
    "        # GA-specific index columns\n",
    "        ga_index_columns = [\n",
    "            \"undl_id\", \"date\", \"session\", \"resolution\", \"draft\", \n",
    "            \"committee_report\", \"meeting\", \"title\", \"agenda_title\", \n",
    "            \"subjects\", \"total_yes\", \"total_no\", \"total_abstentions\", \n",
    "            \"total_non_voting\", \"total_ms\", \"undl_link\"\n",
    "        ]\n",
    "        \n",
    "        # Transform to resolution-per-row format\n",
    "        transformed_df = raw_data.pivot(\n",
    "            index=ga_index_columns, \n",
    "            columns='ms_code', \n",
    "            values='ms_vote'\n",
    "        ).reset_index()\n",
    "        transformed_df.columns.name = None\n",
    "        \n",
    "        self.logger.info(f\"Transformed {len(transformed_df)} GA resolutions\")\n",
    "        \n",
    "        # Parse subjects using GA-specific logic\n",
    "        print(kwargs)\n",
    "        subject_table = kwargs.get('subject_table')\n",
    "        if subject_table is not None and not subject_table.empty:\n",
    "            parsed_df = self._parse_subjects(transformed_df, subject_table)\n",
    "            self.logger.info(f\"Processed {len(parsed_df)} GA resolutions with parsed subjects\")\n",
    "            return {\"ga_resolutions\": parsed_df}\n",
    "        else:\n",
    "            self.logger.warning(\"No subject_table provided, skipping subject parsing\")\n",
    "            return {\"ga_resolutions\": transformed_df}\n",
    "    \n",
    "    def _parse_subjects(self, df: pd.DataFrame, subject_table: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parse the 'subjects' column in the DataFrame to create a separate row for each subject.\n",
    "        Uses GA-specific parsing grammar.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing a 'subjects' column with special parsing grammar.\n",
    "            subject_table (pd.DataFrame): Subject table for matching subject IDs.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with each subject in a separate row and subject_id mapping.\n",
    "        \n",
    "        Notes:\n",
    "            The parsing grammar includes splitting at | and --\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Parsing GA subjects using | and -- delimiters\")\n",
    "        \n",
    "        # Store original subject count for logging\n",
    "        original_subject_count = len(df['subjects'].unique()) if 'subjects' in df.columns else 0\n",
    "        \n",
    "        # First we split the subjects by | and explode the list into separate rows\n",
    "        df_expanded = df.assign(subjects=df['subjects'].str.split('|')).explode('subjects')\n",
    "\n",
    "        # For now we just take the first element if --\n",
    "        df_expanded = df_expanded.assign(subjects=df_expanded['subjects'].str.split('--').str[0]).explode('subjects')\n",
    "\n",
    "        # Clean up subjects (strip whitespace)\n",
    "        df_expanded['subjects'] = df_expanded['subjects'].str.strip()\n",
    "        \n",
    "        # Remove empty subjects\n",
    "        #df_expanded = df_expanded[df_expanded['subjects'].notna() & (df_expanded['subjects'] != '')]\n",
    "        \n",
    "        final_subject_count = len(df_expanded['subjects'].unique()) if 'subjects' in df_expanded.columns else 0\n",
    "        self.logger.info(f\"Expanded subjects from {original_subject_count} to {final_subject_count} unique subjects.\")\n",
    "        \n",
    "        # Add subject IDs by matching with subject_table\n",
    "        df_with_ids = self._add_subject_ids(df_expanded, subject_table)\n",
    "        \n",
    "        return df_with_ids\n",
    "    \n",
    "    def _add_subject_ids(self, resolution_df: pd.DataFrame, subjects_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add subject_id column to resolutions dataframe by matching subject strings.\n",
    "        \n",
    "        Args:\n",
    "            resolution_df: DataFrame with resolutions, already parsed to one row per resolution-subject pair\n",
    "            subjects_df: DataFrame with subject_id and multilingual labels\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Original resolution_df with added 'subject_id' column\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Matching GA subjects to subject IDs\")\n",
    "        \n",
    "        # Build matching index from subjects_df\n",
    "        matching_index = {}\n",
    "        \n",
    "        for _, row in subjects_df.iterrows():\n",
    "            subject_id = row['subject_id']\n",
    "            \n",
    "            # Index all labels\n",
    "            for col in subjects_df.columns:\n",
    "                if col.startswith('label_') and pd.notna(row[col]):\n",
    "                    label_lower = str(row[col]).lower().strip()\n",
    "                    matching_index[label_lower] = subject_id\n",
    "                        \n",
    "                # Also index alternative labels if they exist\n",
    "                if col.startswith('alt_labels_') and pd.notna(row[col]) and str(row[col]).strip():\n",
    "                    for alt_label in str(row[col]).split(';'):\n",
    "                        alt_label_lower = alt_label.lower().strip()\n",
    "                        if alt_label_lower:\n",
    "                            matching_index[alt_label_lower] = subject_id\n",
    "        \n",
    "        # Map subjects to subject_ids\n",
    "        def map_subject(subject_string):\n",
    "            if pd.isna(subject_string):\n",
    "                return None\n",
    "            subject_clean = str(subject_string).lower().strip()\n",
    "            return matching_index.get(subject_clean, None)\n",
    "        \n",
    "        # Add the subject_id column\n",
    "        resolution_df['subject_id'] = resolution_df['subjects'].apply(map_subject)\n",
    "        \n",
    "        # Report statistics\n",
    "        total_rows = len(resolution_df)\n",
    "        matched_rows = resolution_df['subject_id'].notna().sum()\n",
    "        unmatched_rows = resolution_df['subject_id'].isna().sum()\n",
    "        \n",
    "        self.logger.info(f\"GA Subject Mapping Results:\")\n",
    "        self.logger.info(f\"  Total rows: {total_rows}\")\n",
    "        self.logger.info(f\"  Matched: {matched_rows} ({matched_rows/total_rows*100:.1f}%)\")\n",
    "        self.logger.info(f\"  Unmatched: {unmatched_rows} ({unmatched_rows/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        if unmatched_rows > 0:\n",
    "            self.logger.info(f\"Sample unmatched GA subjects:\")\n",
    "            unmatched_samples = resolution_df[resolution_df['subject_id'].isna()]['subjects'].drop_duplicates().head(5)\n",
    "            for subject in unmatched_samples:\n",
    "                self.logger.info(f\"  - '{subject}'\")\n",
    "        \n",
    "        return resolution_df\n",
    "    \n",
    "    def get_dataset_type(self) -> str:\n",
    "        return \"ga_resolutions\"\n",
    "\n",
    "# Implementations for SC resolutions\n",
    "class SCResolutionFetcher(DatasetFetcher):\n",
    "    \"\"\"Fetches Security Council resolution data\"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def fetch(self, source_config: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"Fetch SC resolution data from URL.\"\"\"\n",
    "        self.logger.info(f\"Fetching SC resolutions from {source_config['url']}\")\n",
    "        try:\n",
    "            df = pd.read_csv(source_config['url'])\n",
    "            self.logger.info(f\"Successfully fetched {len(df)} SC resolution records\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to fetch SC resolutions: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def get_dataset_type(self) -> str:\n",
    "        return \"sc_resolutions\"\n",
    "\n",
    "class SCResolutionProcessor(DatasetProcessor):\n",
    "    \"\"\"Processes Security Council resolution data.\"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "      \n",
    "    def process(self, raw_data: pd.DataFrame, **kwargs) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Transform SC data from country-per-row to resolution-per-row format.\"\"\"\n",
    "        self.logger.info(\"Processing SC resolution data\")\n",
    "\n",
    "        # SC-specific index columns\n",
    "        sc_index_cols = [\n",
    "            \"undl_id\", \"date\", \"resolution\", \"draft\", \"meeting\", \"description\", \n",
    "            \"agenda\", \"subjects\", \"modality\", \"total_yes\", \"total_no\", \n",
    "            \"total_abstentions\", \"total_non_voting\", \"total_ms\", \"undl_link\"\n",
    "        ]\n",
    "\n",
    "        # Transform to resolution-per-row format\n",
    "        transformed_df = raw_data.pivot(\n",
    "            index=sc_index_cols,\n",
    "            columns='ms_code',\n",
    "            values='ms_vote'\n",
    "        ).reset_index()\n",
    "\n",
    "        transformed_df.columns.name = None\n",
    "\n",
    "        self.logger.info(f\"Processed {len(transformed_df)} SC resolutions\")\n",
    "        return {'sc_resolutions': transformed_df}\n",
    "    \n",
    "    def get_dataset_type(self) -> str:\n",
    "        return 'sc_resolutions'\n",
    "\n",
    "    \n",
    "# Thesaurus handling (separate from resolution datasets)\n",
    "class ThesaurusFetcher:\n",
    "    \"\"\"Fetches thesaurus data (RDF/TTL format).\"\"\"\n",
    "    \n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def fetch(self, source_config: Dict[str, Any]) -> Graph:\n",
    "        \"\"\"Fetch and parse thesaurus graph.\"\"\"\n",
    "        self.logger.info(f\"Fetching thesaurus from {source_config['url']}\")\n",
    "\n",
    "        thesaurus_url = source_config['url']\n",
    "\n",
    "        graph = Graph()\n",
    "\n",
    "        try:\n",
    "            response = requests.get(thesaurus_url)\n",
    "        except Exception as e:\n",
    "            self.logger.info(\"Error fetching thesaurus file. The dataset might has been updated. Check the date in the URL.\")\n",
    "            self.logger.info(\"Thesaurus URL:\", thesaurus_url)\n",
    "            self.logger.info(f\"Error: {e}\")\n",
    "            \n",
    "            raise ValueError(\"Failed to fetch thesaurus\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            self.logger.info(\"Error fetching thesaurus file. The dataset might has been updated. Check the date in the URL. Response code:\", response.status_code)\n",
    "            self.logger.info(\"Thesaurus URL:\", thesaurus_url)\n",
    "\n",
    "            raise ValueError(\"Failed to fetch thesaurus\")\n",
    "        \n",
    "        ttl_content = BytesIO(response.content)\n",
    "        self.logger.info(f\"Downloaded thesaurus file successfully\")\n",
    "\n",
    "        graph.parse(ttl_content, format=\"turtle\")\n",
    "\n",
    "        return graph\n",
    "\n",
    "class ThesaurusProcessor:\n",
    "    \"\"\"Processes thesaurus data into subject and closure tables.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def process(self, thesaurus_graph: Graph) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Process thesaurus into subject and closure tables.\"\"\"\n",
    "        self.logger.info(\"Processing thesaurus data\")\n",
    "        \n",
    "        # Extract subjects data with multilingual labels\n",
    "        subjects_data = self._extract_subjects_data(thesaurus_graph)\n",
    "        \n",
    "        # Create subjects table\n",
    "        subject_table = self._create_subjects_table(subjects_data)\n",
    "        \n",
    "        # Create closure table for hierarchical relationships\n",
    "        closure_table = self._create_closure_table(thesaurus_graph)\n",
    "        \n",
    "        return {\n",
    "            \"subject_table\": subject_table,\n",
    "            \"closure_table\": closure_table\n",
    "        }\n",
    "    \n",
    "    def _extract_subjects_data(self, g: Graph) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract all subjects and schemes with their multilingual labels from the thesaurus.\n",
    "        \n",
    "        Args:\n",
    "            g (Graph): RDFLib Graph containing the thesaurus data.\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Dictionary with subject_id as key and metadata as value\n",
    "        \"\"\"\n",
    "        subjects_data = {}\n",
    "        \n",
    "        # First, get all SKOS Concepts\n",
    "        for subject in g.subjects(RDF.type, SKOS.Concept):\n",
    "            subject_id = str(subject)\n",
    "            \n",
    "            if subject_id not in subjects_data:\n",
    "                subjects_data[subject_id] = {\n",
    "                    'subject_id': subject_id,\n",
    "                    'labels': {},\n",
    "                    'alt_labels': {},\n",
    "                    'node_type': 'concept'\n",
    "                }\n",
    "            \n",
    "            # Get all prefLabels with language tags\n",
    "            for label in g.objects(subject, SKOS.prefLabel):\n",
    "                if hasattr(label, 'language') and label.language:\n",
    "                    subjects_data[subject_id]['labels'][label.language] = str(label)\n",
    "                else:\n",
    "                    subjects_data[subject_id]['labels']['unknown'] = str(label)\n",
    "            \n",
    "            # Get alternative labels\n",
    "            for alt_label in g.objects(subject, SKOS.altLabel):\n",
    "                if hasattr(alt_label, 'language') and alt_label.language:\n",
    "                    if alt_label.language not in subjects_data[subject_id]['alt_labels']:\n",
    "                        subjects_data[subject_id]['alt_labels'][alt_label.language] = []\n",
    "                    subjects_data[subject_id]['alt_labels'][alt_label.language].append(str(alt_label))\n",
    "        \n",
    "        # Now, get all Concept Schemes (top level containers)\n",
    "        for scheme in g.subjects(RDF.type, SKOS.ConceptScheme):\n",
    "            scheme_id = str(scheme)\n",
    "            \n",
    "            if scheme_id not in subjects_data:\n",
    "                subjects_data[scheme_id] = {\n",
    "                    'subject_id': scheme_id,\n",
    "                    'labels': {},\n",
    "                    'alt_labels': {},\n",
    "                    'node_type': 'scheme'\n",
    "                }\n",
    "            \n",
    "            # Get all prefLabels for schemes\n",
    "            for label in g.objects(scheme, SKOS.prefLabel):\n",
    "                if hasattr(label, 'language') and label.language:\n",
    "                    subjects_data[scheme_id]['labels'][label.language] = str(label)\n",
    "                else:\n",
    "                    subjects_data[scheme_id]['labels']['unknown'] = str(label)\n",
    "        \n",
    "        self.logger.info(f\"Extracted {len(subjects_data)} subjects/schemes:\")\n",
    "        self.logger.info(f\"  - Concepts: {sum(1 for d in subjects_data.values() if d.get('node_type') == 'concept')}\")\n",
    "        self.logger.info(f\"  - Schemes: {sum(1 for d in subjects_data.values() if d.get('node_type') == 'scheme')}\")\n",
    "        \n",
    "        return subjects_data\n",
    "\n",
    "    def _create_subjects_table(self, subjects_data: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert subjects dictionary to a DataFrame with language columns.\n",
    "        \n",
    "        Args:\n",
    "            subjects_data (Dict): Dictionary containing subject metadata.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with columns for subject_id, labels in different languages, alt_labels, and node_type.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        # Common language codes in UNBIS\n",
    "        languages = ['en', 'es', 'fr', 'ar', 'ru', 'zh']\n",
    "        \n",
    "        for subject_id, data in subjects_data.items():\n",
    "            row = {'subject_id': subject_id}\n",
    "            \n",
    "            # Add labels for each language\n",
    "            for lang in languages:\n",
    "                row[f'label_{lang}'] = data['labels'].get(lang, None)\n",
    "                # Store alt labels as semicolon-separated string\n",
    "                alt_labels = data['alt_labels'].get(lang, [])\n",
    "                row[f'alt_labels_{lang}'] = '; '.join(alt_labels) if alt_labels else ''\n",
    "            \n",
    "            # Add node type (concept or scheme)\n",
    "            row['node_type'] = data.get('node_type', 'concept')\n",
    "            \n",
    "            rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def _create_closure_table(self, g: Graph) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a closure table directly from RDFLib Graph using SKOS relationships.\n",
    "        \n",
    "        This function traverses the SKOS broader/narrower relationships to build\n",
    "        a closure table without converting to NetworkX first.\n",
    "        \n",
    "        Args:\n",
    "            g (Graph): RDFLib Graph containing the thesaurus with SKOS relationships\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Closure table with columns:\n",
    "                - ancestor_id (str): URI of the ancestor subject/scheme\n",
    "                - descendant_id (str): URI of the descendant subject/scheme  \n",
    "                - depth (int): Number of edges between ancestor and descendant\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, build adjacency list from SKOS broader relationships\n",
    "        # broader means parent, so child -> parent edges\n",
    "        children_to_parents = defaultdict(set)\n",
    "        parents_to_children = defaultdict(set)\n",
    "        \n",
    "        # Get all subjects (concepts and schemes)\n",
    "        all_subjects = set()\n",
    "        \n",
    "        # Add concepts\n",
    "        for concept in g.subjects(RDF.type, SKOS.Concept):\n",
    "            all_subjects.add(str(concept))\n",
    "        \n",
    "        # Add schemes\n",
    "        for scheme in g.subjects(RDF.type, SKOS.ConceptScheme):\n",
    "            all_subjects.add(str(scheme))\n",
    "        \n",
    "        # Build parent-child relationships from broader\n",
    "        for child, parent in g.subject_objects(SKOS.broader):\n",
    "            child_str = str(child)\n",
    "            parent_str = str(parent)\n",
    "            children_to_parents[child_str].add(parent_str)\n",
    "            parents_to_children[parent_str].add(child_str)\n",
    "            all_subjects.add(child_str)\n",
    "            all_subjects.add(parent_str)\n",
    "        \n",
    "        # Add scheme relationships (top concepts)\n",
    "        for scheme in g.subjects(RDF.type, SKOS.ConceptScheme):\n",
    "            scheme_str = str(scheme)\n",
    "            # Get top concepts\n",
    "            for top_concept in g.objects(scheme, SKOS.hasTopConcept):\n",
    "                concept_str = str(top_concept)\n",
    "                children_to_parents[concept_str].add(scheme_str)\n",
    "                parents_to_children[scheme_str].add(concept_str)\n",
    "        \n",
    "        self.logger.info(f\"Found {len(all_subjects)} subjects/schemes\")\n",
    "        self.logger.info(f\"Found {sum(len(p) for p in children_to_parents.values())} parent-child relationships\")\n",
    "        \n",
    "        # Now build closure table\n",
    "        closure_data = []\n",
    "        \n",
    "        def get_all_ancestors(subject_id):\n",
    "            \"\"\"BFS to find all ancestors and their depths\"\"\"\n",
    "            ancestors = {}\n",
    "            visited = set()\n",
    "            queue = deque([(subject_id, 0)])\n",
    "            \n",
    "            while queue:\n",
    "                current, depth = queue.popleft()\n",
    "                \n",
    "                if current in visited:\n",
    "                    continue\n",
    "                visited.add(current)\n",
    "                \n",
    "                # Add current as ancestor at this depth (for self-reference at depth 0)\n",
    "                if current not in ancestors or ancestors[current] > depth:\n",
    "                    ancestors[current] = depth\n",
    "                \n",
    "                # Add parents to queue\n",
    "                for parent in children_to_parents.get(current, []):\n",
    "                    if parent not in visited:\n",
    "                        queue.append((parent, depth + 1))\n",
    "            \n",
    "            return ancestors\n",
    "        \n",
    "        # Process each subject\n",
    "        nodes_with_ancestors = 0\n",
    "        for subject in all_subjects:\n",
    "            ancestors = get_all_ancestors(subject)\n",
    "            \n",
    "            if len(ancestors) > 1:  # More than just self\n",
    "                nodes_with_ancestors += 1\n",
    "            \n",
    "            for ancestor, depth in ancestors.items():\n",
    "                closure_data.append({\n",
    "                    'ancestor_id': ancestor,\n",
    "                    'descendant_id': subject,\n",
    "                    'depth': depth\n",
    "                })\n",
    "        \n",
    "        closure_df = pd.DataFrame(closure_data)\n",
    "        \n",
    "        # self.logger.info(f\"Closure table statistics:\")\n",
    "        # self.logger.info(f\"  Total rows: {len(closure_df):,}\")\n",
    "        # self.logger.info(f\"  Nodes with ancestors: {nodes_with_ancestors:,}\")\n",
    "        # self.logger.info(f\"  Unique ancestors: {closure_df['ancestor_id'].nunique():,}\")\n",
    "        # self.logger.info(f\"  Unique descendants: {closure_df['descendant_id'].nunique():,}\")\n",
    "        # if len(closure_df) > 0:\n",
    "        #     self.logger.info(f\"  Max depth: {closure_df['depth'].max()}\")\n",
    "            \n",
    "        #     # Show depth distribution\n",
    "        #     depth_counts = closure_df['depth'].value_counts().sort_index()\n",
    "        #     self.logger.info(f\"Depth distribution:\")\n",
    "        #     for depth, count in depth_counts.items():\n",
    "        #         self.logger.info(f\"  Depth {depth}: {count:,} relationships\")\n",
    "        \n",
    "        return closure_df\n",
    "\n",
    "class DataFetcher:\n",
    "    \"\"\"Orchestrates fetching from multiple data sources.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any], logger: logging.Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        \n",
    "        # Registry of dataset fetchers\n",
    "        self._dataset_fetchers: Dict[str, DatasetFetcher] = {}\n",
    "        self._register_default_fetchers()\n",
    "        \n",
    "        # Thesaurus fetcher (separate from datasets)\n",
    "        self.thesaurus_fetcher = ThesaurusFetcher(logger)\n",
    "    \n",
    "    def _register_default_fetchers(self):\n",
    "        \"\"\"Register default dataset fetchers.\"\"\"\n",
    "        ga_fetcher = GAResolutionFetcher(self.logger)\n",
    "        self._dataset_fetchers[ga_fetcher.get_dataset_type()] = ga_fetcher\n",
    "        \n",
    "        # Future fetchers can be added here:\n",
    "        sc_fetcher = SCResolutionFetcher(self.logger)\n",
    "        self._dataset_fetchers[sc_fetcher.get_dataset_type()] = sc_fetcher\n",
    "    \n",
    "    def register_fetcher(self, fetcher: DatasetFetcher):\n",
    "        \"\"\"Register a new dataset fetcher.\"\"\"\n",
    "        self._dataset_fetchers[fetcher.get_dataset_type()] = fetcher\n",
    "    \n",
    "    def fetch_resolutions(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Fetch all configured resolution datasets.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for dataset_type, source_config in self.config['data_sources']['resolutions'].items():\n",
    "                \n",
    "            if dataset_type in self._dataset_fetchers:\n",
    "                fetcher = self._dataset_fetchers[dataset_type]\n",
    "                results[dataset_type] = fetcher.fetch(source_config)\n",
    "            else:\n",
    "                self.logger.warning(f\"No fetcher registered for dataset type: {dataset_type}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def fetch_thesaurus(self) -> Graph:\n",
    "        \"\"\"Fetch thesaurus data.\"\"\"\n",
    "        thesaurus_config = self.config['data_sources'].get('thesaurus')\n",
    "        if not thesaurus_config:\n",
    "            raise ValueError(\"No thesaurus configuration found\")\n",
    "        \n",
    "        return self.thesaurus_fetcher.fetch(thesaurus_config)\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Orchestrates processing of individual datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any], logger: logging.Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        \n",
    "        # Registry of dataset processors\n",
    "        self._dataset_processors: Dict[str, DatasetProcessor] = {}\n",
    "        self._register_default_processors()\n",
    "        \n",
    "        # Thesaurus processor (separate from datasets)\n",
    "        self.thesaurus_processor = ThesaurusProcessor(logger)\n",
    "    \n",
    "    def _register_default_processors(self):\n",
    "        \"\"\"Register default dataset processors.\"\"\"\n",
    "        ga_processor = GAResolutionProcessor(self.logger)\n",
    "        self._dataset_processors[ga_processor.get_dataset_type()] = ga_processor\n",
    "        \n",
    "        # Future processors can be added here:\n",
    "        sc_processor = SCResolutionProcessor(self.logger)\n",
    "        self._dataset_processors[sc_processor.get_dataset_type()] = sc_processor\n",
    "    \n",
    "    def register_processor(self, processor: DatasetProcessor):\n",
    "        \"\"\"Register a new dataset processor.\"\"\"\n",
    "        self._dataset_processors[processor.get_dataset_type()] = processor\n",
    "    \n",
    "    def process_resolutions(self, raw_datasets: Dict[str, pd.DataFrame], **kwargs) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Process individual resolution datasets.\"\"\"\n",
    "        processed_datasets = {}\n",
    "        \n",
    "        for dataset_type, raw_data in raw_datasets.items():\n",
    "            if dataset_type in self._dataset_processors:\n",
    "                processor = self._dataset_processors[dataset_type]\n",
    "                processed_data = processor.process(raw_data, **kwargs)\n",
    "                processed_datasets.update(processed_data)\n",
    "            else:\n",
    "                self.logger.warning(f\"No processor registered for dataset type: {dataset_type}\")\n",
    "        \n",
    "        return processed_datasets\n",
    "    \n",
    "    def process_thesaurus(self, thesaurus_graph: Graph) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Process thesaurus data.\"\"\"\n",
    "        return self.thesaurus_processor.process(thesaurus_graph)\n",
    "    \n",
    "    def normalize_resolutions(self, resolutions_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Normalize the resolutions dataframe into separate tables.\n",
    "        \n",
    "        Args:\n",
    "            resolutions_df : pd.DataFrame\n",
    "                DataFrame with one row per resolution-subject pair, containing\n",
    "                resolution metadata and subject_id column\n",
    "            \n",
    "        Returns:\n",
    "            tuple of (resolutions_normalized_df, resolution_subjects_df)\n",
    "                - resolutions_normalized_df: One row per resolution with metadata\n",
    "                - resolution_subjects_df: Resolution-subject pairs mapping table\n",
    "        \"\"\"\n",
    "        \n",
    "        # Identify columns that belong to resolution metadata vs subject mapping\n",
    "        subject_columns = ['subjects', 'subject_id']\n",
    "        resolution_columns = [col for col in resolutions_df.columns if col not in subject_columns]\n",
    "        \n",
    "        # 1. Create normalized resolutions table (one row per resolution)\n",
    "        resolutions_normalized_df = resolutions_df[resolution_columns].drop_duplicates()\n",
    "        \n",
    "        # 2. Create resolution-subject mapping table\n",
    "        # Only keep rows with valid subject_ids\n",
    "        print(resolutions_df.columns.tolist())\n",
    "        valid_mappings = resolutions_df[resolutions_df['subject_id'].notna()]\n",
    "        resolution_subjects_df = valid_mappings[['undl_id', 'subject_id']].copy()\n",
    "        \n",
    "        # Remove duplicates (in case same subject appears multiple times for a resolution)\n",
    "        resolution_subjects_df = resolution_subjects_df.drop_duplicates()\n",
    "        \n",
    "        # 3. Report statistics\n",
    "        # self.logger.info(\"Normalization Results:\")\n",
    "        # self.logger.info(f\"\\nResolutions Table:\")\n",
    "        # self.logger.info(f\"  Total resolutions: {len(resolutions_normalized_df)}\")\n",
    "        # self.logger.info(f\"  Columns: {', '.join(resolutions_normalized_df.columns)}\")\n",
    "        \n",
    "        # self.logger.info(f\"\\nResolution-Subject Mapping Table:\")\n",
    "        # self.logger.info(f\"  Total mappings: {len(resolution_subjects_df)}\")\n",
    "        # self.logger.info(f\"  Unique resolutions with subjects: {resolution_subjects_df['undl_id'].nunique()}\")\n",
    "        # self.logger.info(f\"  Unique subjects used: {resolution_subjects_df['subject_id'].nunique()}\")\n",
    "        # self.logger.info(f\"  Avg subjects per resolution: {len(resolution_subjects_df) / resolution_subjects_df['undl_id'].nunique():.2f}\")\n",
    "        \n",
    "        # Check for resolutions without subjects\n",
    "        resolutions_without_subjects = set(resolutions_normalized_df['undl_id']) - set(resolution_subjects_df['undl_id'])\n",
    "        if resolutions_without_subjects:\n",
    "            self.logger.info(f\"\\nWarning: {len(resolutions_without_subjects)} resolutions have no mapped subjects\")\n",
    "        \n",
    "        return resolutions_normalized_df, resolution_subjects_df\n",
    "\n",
    "class DataMerger:\n",
    "    \"\"\"Handles merging multiple resolution datasets into unified formats.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def _get_unified_schema(self) -> Dict[str, str]:\n",
    "        \"\"\"Define the unified schema for the merged resolutions table.\"\"\"\n",
    "        return {\n",
    "            # Core identifier fields (present in all datasets)\n",
    "            'undl_id': 'str',\n",
    "            'date': 'datetime',\n",
    "            'resolution': 'str',\n",
    "            'draft': 'str',\n",
    "            'meeting': 'str',\n",
    "            'subjects': 'str',\n",
    "            'undl_link': 'str',\n",
    "            \n",
    "            # Voting summary fields (present in all datasets)\n",
    "            'total_yes': 'int',\n",
    "            'total_no': 'int', \n",
    "            'total_abstentions': 'int',\n",
    "            'total_non_voting': 'int',\n",
    "            'total_ms': 'int',\n",
    "            \n",
    "            # Dataset source identification\n",
    "            'source_dataset': 'str',  # 'GA', 'SC', 'HRC', etc.\n",
    "            \n",
    "            # Content fields (may vary by dataset, nullable)\n",
    "            'title': 'str',           # GA has this, SC uses 'description'\n",
    "            'agenda_title': 'str',    # GA specific\n",
    "            'agenda': 'str',          # SC specific  \n",
    "            'session': 'str',         # GA specific\n",
    "            'committee_report': 'str', # GA specific\n",
    "            'modality': 'str',        # SC specific (voting type)\n",
    "            'description': 'str'      # SC specific\n",
    "        }\n",
    "    \n",
    "    def _normalize_to_unified_schema(self, df: pd.DataFrame, dataset_type: str) -> pd.DataFrame:\n",
    "        \"\"\"Normalize a dataset-specific DataFrame to the unified schema.\"\"\"\n",
    "        self.logger.info(f\"Normalizing {dataset_type} to unified schema\")\n",
    "        \n",
    "        unified_df = pd.DataFrame()\n",
    "        schema = self._get_unified_schema()\n",
    "        \n",
    "        # Add source dataset identifier\n",
    "        unified_df['source_dataset'] = dataset_type.upper().replace('_RESOLUTIONS', '')\n",
    "        \n",
    "        # Map common fields directly\n",
    "        common_fields = ['undl_id', 'date', 'resolution', 'draft', 'meeting', \n",
    "                        'subjects', 'undl_link', 'total_yes', 'total_no', \n",
    "                        'total_abstentions', 'total_non_voting', 'total_ms']\n",
    "        \n",
    "        for field in common_fields:\n",
    "            if field in df.columns:\n",
    "                unified_df[field] = df[field]\n",
    "            else:\n",
    "                self.logger.warning(f\"Missing expected field '{field}' in {dataset_type}\")\n",
    "                unified_df[field] = None\n",
    "        \n",
    "        # Handle dataset-specific field mappings\n",
    "        if dataset_type == 'ga_resolutions':\n",
    "            # GA-specific fields\n",
    "            unified_df['title'] = df.get('title')\n",
    "            unified_df['agenda_title'] = df.get('agenda_title') \n",
    "            unified_df['session'] = df.get('session')\n",
    "            unified_df['committee_report'] = df.get('committee_report')\n",
    "            # Set SC/HRC fields to None\n",
    "            unified_df['description'] = None\n",
    "            unified_df['agenda'] = None\n",
    "            unified_df['modality'] = None\n",
    "            \n",
    "        elif dataset_type == 'sc_resolutions':\n",
    "            # SC-specific fields\n",
    "            unified_df['description'] = df.get('description')\n",
    "            unified_df['agenda'] = df.get('agenda')\n",
    "            unified_df['modality'] = df.get('modality')\n",
    "            # Map SC description to title for consistency\n",
    "            unified_df['title'] = df.get('description')\n",
    "            # Set GA fields to None\n",
    "            unified_df['agenda_title'] = None\n",
    "            unified_df['session'] = None\n",
    "            unified_df['committee_report'] = None\n",
    "        \n",
    "        # Copy voting columns (country votes)\n",
    "        voting_columns = [col for col in df.columns if col not in schema.keys()]\n",
    "        for col in voting_columns:\n",
    "            unified_df[col] = df[col]\n",
    "        \n",
    "        self.logger.info(f\"Normalized {len(unified_df)} {dataset_type} records\")\n",
    "        return unified_df\n",
    "    \n",
    "    def merge_resolutions(self, processed_datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"Merge multiple resolution datasets into a unified table.\"\"\"\n",
    "        self.logger.info(\"Merging resolution datasets into unified table\")\n",
    "        \n",
    "        unified_datasets = []\n",
    "        \n",
    "        for dataset_type, df in processed_datasets.items():\n",
    "            if dataset_type.endswith('_resolutions'):\n",
    "                normalized_df = self._normalize_to_unified_schema(df, dataset_type)\n",
    "                unified_datasets.append(normalized_df)\n",
    "                self.logger.info(f\"Added {len(normalized_df)} records from {dataset_type}\")\n",
    "        \n",
    "        if not unified_datasets:\n",
    "            self.logger.warning(\"No resolution datasets found to merge\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Concatenate all datasets\n",
    "        merged_df = pd.concat(unified_datasets, ignore_index=True, sort=False)\n",
    "        \n",
    "        # Sort by date and source for consistent ordering\n",
    "        merged_df = merged_df.sort_values(['date', 'source_dataset', 'undl_id'])\n",
    "        \n",
    "        self.logger.info(f\"Successfully merged {len(merged_df)} total resolutions from {len(unified_datasets)} datasets\")\n",
    "        return merged_df\n",
    "\n",
    "class DataRepository:\n",
    "    \"\"\"Handles storage and retrieval of processed UN data.\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        self.config_path = config_path\n",
    "        \n",
    "        # Initialize data attributes\n",
    "        self.resolution_table: pd.DataFrame\n",
    "        self.resolution_subject_table: pd.DataFrame\n",
    "        self.subject_table: pd.DataFrame\n",
    "        self.closure_table: pd.DataFrame\n",
    "\n",
    "        # Load configuration\n",
    "        self._load_config()\n",
    "\n",
    "        # Initialize Logging\n",
    "        self._setup_logging()\n",
    "\n",
    "        self.logger.info(\"Initializing UNDataRepository\")\n",
    "\n",
    "        # Check if links are still valid\n",
    "        if not self._check_URLS():\n",
    "            self.logger.error(\"One or more data URLs are invalid. The dataset might have been updated. Check the date in the URL.\")\n",
    "            raise ValueError(\"Invalid data URLs in configuration.\")\n",
    "        \n",
    "        # Check if data is already processed and available\n",
    "        if self._has_cached_data():\n",
    "            # Cached data found -> load\n",
    "            self._load_cached_data() \n",
    "            self.logger.info(\"Initialization Complete with Cached Data.\")\n",
    "            return\n",
    "        \n",
    "        self._build_data()\n",
    "        self.logger.info(\"Initialization complete with fetched Data.\")\n",
    "\n",
    "    def get_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Return processed data as a dictionary of DataFrames.\"\"\"\n",
    "        return {\n",
    "            'resolution': self.resolution_table,\n",
    "            'resolution_subject': self.resolution_subject_table,\n",
    "            'subject': self.subject_table,\n",
    "            'closure': self.closure_table\n",
    "        }\n",
    "    \n",
    "    def _load_config(self):\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        with open(self.config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"\n",
    "        Setup logging configuration with file and console handlers.\n",
    "\n",
    "        \"\"\"\n",
    "        # Create logger\n",
    "        self.logger = logging.getLogger('UNResolutionAnalyzer')\n",
    "        \n",
    "        if not self.config['logs']:\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "\n",
    "        self.logger.setLevel(logging.DEBUG if self.config['debug'] else logging.INFO)\n",
    "\n",
    "        # Clear any existing handlers\n",
    "        self.logger.handlers.clear()\n",
    "\n",
    "        # Create formatters\n",
    "        detailed_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n",
    "        )\n",
    "        simple_formatter = logging.Formatter(\n",
    "            '%(levelname)s - %(message)s'\n",
    "        )\n",
    "\n",
    "        log_dir = Path(self.config['paths']['logs'])\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "        log_file = log_dir / 'un_resolution_analyzer.log'\n",
    "\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(detailed_formatter)\n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "        # Console handler\n",
    "        if self.config['debug']:\n",
    "            console_handler = logging.StreamHandler(sys.stdout)\n",
    "            console_handler.setLevel(logging.DEBUG)\n",
    "            console_handler.setFormatter(simple_formatter)\n",
    "            self.logger.addHandler(console_handler)\n",
    "\n",
    "        self.logger.info(\"Logging setup complete.\")\n",
    "\n",
    "    def _check_URLS(self) -> bool:\n",
    "        \"\"\"Check if the URLs in the configuration are reachable.\"\"\"\n",
    "        urls = [\n",
    "            source['url'] \n",
    "            for source in self.config['data_sources'].values() \n",
    "            if 'url' in source\n",
    "        ]\n",
    "        all_valid = True\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.head(url, allow_redirects=True, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    self.logger.error(f\"URL not reachable: {url} (Status code: {response.status_code})\")\n",
    "                    all_valid = False\n",
    "                else:\n",
    "                    self.logger.info(f\"URL is valid: {url}\")\n",
    "            except requests.RequestException as e:\n",
    "                self.logger.error(f\"Error reaching URL: {url} ({e})\")\n",
    "                all_valid = False\n",
    "        return all_valid\n",
    "    \n",
    "    def _has_cached_data(self) -> bool:\n",
    "        \"\"\"Check if cached data files exist.\"\"\"\n",
    "        data_path = Path(self.config['paths']['data'])\n",
    "        required_files = [\n",
    "            'resolution_table.csv',\n",
    "            'resolution_subject_table.csv',\n",
    "            'subject_table.csv',\n",
    "            'closure_table.csv'\n",
    "        ]\n",
    "        all_exist = all((data_path / file).exists() for file in required_files)\n",
    "        if all_exist:\n",
    "            self.logger.info(\"Cached Data files found.\")\n",
    "        else:\n",
    "            self.logger.info(\"Cached Data files not found.\")\n",
    "        return all_exist\n",
    "    \n",
    "    def _load_cached_data(self):\n",
    "        \"\"\"Load cached data files into DataFrames.\"\"\"\n",
    "        data_path = Path(self.config['paths']['data'])\n",
    "        data_path.mkdir(exist_ok=True)\n",
    "        self.resolution_table = pd.read_csv(data_path / 'resolution_table.csv')\n",
    "        self.resolution_subject_table = pd.read_csv(data_path / 'resolution_subject_table.csv')\n",
    "        self.subject_table = pd.read_csv(data_path / 'subject_table.csv')\n",
    "        self.closure_table = pd.read_csv(data_path / 'closure_table.csv')\n",
    "        self.logger.info(\"Cached data loaded successfully.\")\n",
    "    \n",
    "    def _save_cached_data(self):\n",
    "        \"\"\"Save data files into DataFrames.\"\"\"\n",
    "        data_path = Path(self.config['paths']['data'])\n",
    "        data_path.mkdir(exist_ok=True)\n",
    "        # Save the tables in the defined folder\n",
    "        self.resolution_table.to_csv(data_path / 'resolution_table.csv', index=False)\n",
    "        self.resolution_subject_table.to_csv(data_path / 'resolution_subject_table.csv', index=False)\n",
    "        self.subject_table.to_csv(data_path / 'subject_table.csv', index=False)\n",
    "        self.closure_table.to_csv(data_path / 'closure_table.csv', index=False)\n",
    "\n",
    "    def _build_data(self):\n",
    "        \"\"\"Build processed data tables from raw sources.\"\"\"\n",
    "        \n",
    "        # Fetch Data\n",
    "        fetcher = DataFetcher(self.config, self.logger)\n",
    "        resolutions_raw = fetcher.fetch_resolutions()\n",
    "        thesaurus_graph = fetcher.fetch_thesaurus()\n",
    "\n",
    "        # Process data\n",
    "        processor = DataProcessor(self.config, self.logger)\n",
    "        merger = DataMerger(self.logger)\n",
    "        \n",
    "        # Process thesaurus first (needed for subject matching)\n",
    "        thesaurus_tables = processor.process_thesaurus(thesaurus_graph)\n",
    "        self.subject_table = thesaurus_tables.get('subject_table', pd.DataFrame())\n",
    "        self.closure_table = thesaurus_tables.get('closure_table', pd.DataFrame())\n",
    "        \n",
    "        # Process individual resolution datasets\n",
    "        processed_datasets = processor.process_resolutions(resolutions_raw, subject_table=self.subject_table)\n",
    "        \n",
    "        # Continue with GA for now\n",
    "        # unified_resolutions = merger.merge_resolutions(processed_datasets)\n",
    "        ga_resolutions = processed_datasets.get('ga_resolutions', pd.DataFrame())\n",
    "\n",
    "        # Normalize ga_resolutions\n",
    "        self.resolution_table, self.resolution_subject_table = processor.normalize_resolutions(ga_resolutions)\n",
    "\n",
    "        # Save processed data\n",
    "        self._save_cached_data()\n",
    "\n",
    "class ResolutionQueryEngine:\n",
    "    \"\"\"Provides querying capabilities on processed resolution data.\"\"\"\n",
    "    \n",
    "    def __init__(self, resolution_df: pd.DataFrame, resolution_subject_df: pd.DataFrame, subject_df: pd.DataFrame, closure_df: pd.DataFrame):\n",
    "        self.resolution_df = resolution_df\n",
    "        self.resolution_subject_df = resolution_subject_df\n",
    "        self.subject_df = subject_df\n",
    "        self.closure_df = closure_df\n",
    "        \n",
    "    def query(self, start_date: Optional[str] = None, end_date: Optional[str] = None, subject_ids: Optional[List[str]] = None, include_descendants: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Query resolutions based on date range and subject filters.\"\"\"\n",
    "        # TODO: Implement querying logic\n",
    "        return pd.DataFrame()\n",
    "\n",
    "class ResolutionAnalyzer:\n",
    "    \"\"\"Analyzes resolution data for trends and patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, resolution_df: pd.DataFrame, resolution_subject_df: pd.DataFrame):\n",
    "        self.resolution_df = resolution_df\n",
    "        self.resolution_subject_df = resolution_subject_df\n",
    "        \n",
    "    def voting_trends(self) -> pd.DataFrame:\n",
    "        \"\"\"Analyze voting trends over time.\"\"\"\n",
    "        # TODO: Implement voting trends analysis\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def subject_distribution(self) -> pd.DataFrame:\n",
    "        \"\"\"Analyze distribution of subjects across resolutions.\"\"\"\n",
    "        # TODO: Implement subject distribution analysis\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39650921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Logging setup complete.\n",
      "INFO - Initializing UNDataRepository\n",
      "INFO - Initializing UNDataRepository\n",
      "INFO - URL is valid: https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Cached Data files not found.\n",
      "INFO - Fetching GA resolutions from https://digitallibrary.un.org/record/4060887/files/2025_9_19_ga_voting.csv\n",
      "INFO - URL is valid: https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Cached Data files not found.\n",
      "INFO - Fetching GA resolutions from https://digitallibrary.un.org/record/4060887/files/2025_9_19_ga_voting.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janic\\AppData\\Local\\Temp\\ipykernel_28504\\3721095617.py:52: DtypeWarning: Columns (5,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(source_config['url'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Successfully fetched 916554 GA resolution records\n",
      "INFO - Fetching thesaurus from https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Fetching thesaurus from https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Downloaded thesaurus file successfully\n",
      "INFO - Downloaded thesaurus file successfully\n",
      "INFO - Processing thesaurus data\n",
      "INFO - Processing thesaurus data\n",
      "INFO - Extracted 7341 subjects/schemes:\n",
      "INFO -   - Concepts: 7322\n",
      "INFO -   - Schemes: 19\n",
      "INFO - Extracted 7341 subjects/schemes:\n",
      "INFO -   - Concepts: 7322\n",
      "INFO -   - Schemes: 19\n",
      "INFO - Found 7341 subjects/schemes\n",
      "INFO - Found 12776 parent-child relationships\n",
      "INFO - Found 7341 subjects/schemes\n",
      "INFO - Found 12776 parent-child relationships\n",
      "INFO - Processing GA resolution data\n",
      "INFO - Processing GA resolution data\n",
      "INFO - Transformed 5534 GA resolutions\n",
      "{'subject_table':                                    subject_id                     label_en  \\\n",
      "0     http://metadata.un.org/thesaurus/030101                    GEOGRAPHY   \n",
      "1     http://metadata.un.org/thesaurus/030102                  CARTOGRAPHY   \n",
      "2     http://metadata.un.org/thesaurus/030103    LAND FORMS AND ECOSYSTEMS   \n",
      "3     http://metadata.un.org/thesaurus/030200          RESOURCES (GENERAL)   \n",
      "4     http://metadata.un.org/thesaurus/030300                  ENVIRONMENT   \n",
      "...                                       ...                          ...   \n",
      "7336      http://metadata.un.org/thesaurus/12                   EMPLOYMENT   \n",
      "7337      http://metadata.un.org/thesaurus/13  HUMANITARIAN AID AND RELIEF   \n",
      "7338      http://metadata.un.org/thesaurus/16       SCIENCE AND TECHNOLOGY   \n",
      "7339      http://metadata.un.org/thesaurus/17     GEOGRAPHICAL DESCRIPTORS   \n",
      "7340      http://metadata.un.org/thesaurus/18     ORGANIZATIONAL QUESTIONS   \n",
      "\n",
      "     alt_labels_en                            label_es alt_labels_es  \\\n",
      "0                                            GEOGRAFIA                 \n",
      "1                                          CARTOGRAFIA                 \n",
      "2                   FORMAS FISIOGRAFICAS Y ECOSISTEMAS                 \n",
      "3                                   RECURSOS NATURALES                 \n",
      "4                                       MEDIO AMBIENTE                 \n",
      "...            ...                                 ...           ...   \n",
      "7336                                            EMPLEO                 \n",
      "7337                            ASISTENCIA HUMANITARIA                 \n",
      "7338                              CIENCIA Y TECNOLOGIA                 \n",
      "7339                          DESCRIPTORES GEOGRAFICOS                 \n",
      "7340                        CUESTIONES DE ORGANIZACION                 \n",
      "\n",
      "                                   label_fr alt_labels_fr  \\\n",
      "0                                GEOGRAPHIE                 \n",
      "1                              CARTOGRAPHIE                 \n",
      "2     FORMATIONS GEOLOGIQUES ET ECOSYSTEMES                 \n",
      "3                     RESSOURCES NATURELLES                 \n",
      "4                             ENVIRONNEMENT                 \n",
      "...                                     ...           ...   \n",
      "7336                                 EMPLOI                 \n",
      "7337                 ASSISTANCE HUMANITAIRE                 \n",
      "7338                 SCIENCES ET TECHNIQUES                 \n",
      "7339             DESCRIPTEURS GEOGRAPHIQUES                 \n",
      "7340           QUESTIONS ORGANISATIONNELLES                 \n",
      "\n",
      "                            label_ar alt_labels_ar  \\\n",
      "0                          الجغرافيا                 \n",
      "1                        رسم الخرائط                 \n",
      "2     أشكال الأرض والنظم الأيكولوجية                 \n",
      "3                       موارد (عامة)                 \n",
      "4                             البيئة                 \n",
      "...                              ...           ...   \n",
      "7336                         العمالة                 \n",
      "7337           معونات إنسانية وإغاثة                 \n",
      "7338              العلم والتكنولوجيا                 \n",
      "7339                  واصفات جغرافية                 \n",
      "7340                   مسائل تنظيمية                 \n",
      "\n",
      "                         label_ru alt_labels_ru     label_zh alt_labels_zh  \\\n",
      "0                       ГЕОГРАФИЯ                         地理                 \n",
      "1                     КАРТОГРАФИЯ                         制图                 \n",
      "2     ОЧЕРТАНИЯ СУШИ И ЭКОСИСТЕМЫ                    地形和生态系统                 \n",
      "3                 РЕСУРСЫ (ОБЩЕЕ)                资源 （一般意义上的）                 \n",
      "4                ОКРУЖАЮЩАЯ СРЕДА                         环境                 \n",
      "...                           ...           ...          ...           ...   \n",
      "7336                       РАБОТА                         就业                 \n",
      "7337          ГУМАНИТАРНАЯ ПОМОЩЬ                     人道主义救援                 \n",
      "7338           НАУКА И ТЕХНОЛОГИЯ                      科学和技术                 \n",
      "7339      ГЕОГРАФИЧЕСКИЕ ОПИСАНИЯ                       地理区划                 \n",
      "7340      ОРГАНИЗАЦИОННЫЕ ВОПРОСЫ                      组织内事务                 \n",
      "\n",
      "     node_type  \n",
      "0      concept  \n",
      "1      concept  \n",
      "2      concept  \n",
      "3      concept  \n",
      "4      concept  \n",
      "...        ...  \n",
      "7336    scheme  \n",
      "7337    scheme  \n",
      "7338    scheme  \n",
      "7339    scheme  \n",
      "7340    scheme  \n",
      "\n",
      "[7341 rows x 14 columns]}\n",
      "INFO - Parsing GA subjects using | and -- delimiters\n",
      "INFO - Transformed 5534 GA resolutions\n",
      "{'subject_table':                                    subject_id                     label_en  \\\n",
      "0     http://metadata.un.org/thesaurus/030101                    GEOGRAPHY   \n",
      "1     http://metadata.un.org/thesaurus/030102                  CARTOGRAPHY   \n",
      "2     http://metadata.un.org/thesaurus/030103    LAND FORMS AND ECOSYSTEMS   \n",
      "3     http://metadata.un.org/thesaurus/030200          RESOURCES (GENERAL)   \n",
      "4     http://metadata.un.org/thesaurus/030300                  ENVIRONMENT   \n",
      "...                                       ...                          ...   \n",
      "7336      http://metadata.un.org/thesaurus/12                   EMPLOYMENT   \n",
      "7337      http://metadata.un.org/thesaurus/13  HUMANITARIAN AID AND RELIEF   \n",
      "7338      http://metadata.un.org/thesaurus/16       SCIENCE AND TECHNOLOGY   \n",
      "7339      http://metadata.un.org/thesaurus/17     GEOGRAPHICAL DESCRIPTORS   \n",
      "7340      http://metadata.un.org/thesaurus/18     ORGANIZATIONAL QUESTIONS   \n",
      "\n",
      "     alt_labels_en                            label_es alt_labels_es  \\\n",
      "0                                            GEOGRAFIA                 \n",
      "1                                          CARTOGRAFIA                 \n",
      "2                   FORMAS FISIOGRAFICAS Y ECOSISTEMAS                 \n",
      "3                                   RECURSOS NATURALES                 \n",
      "4                                       MEDIO AMBIENTE                 \n",
      "...            ...                                 ...           ...   \n",
      "7336                                            EMPLEO                 \n",
      "7337                            ASISTENCIA HUMANITARIA                 \n",
      "7338                              CIENCIA Y TECNOLOGIA                 \n",
      "7339                          DESCRIPTORES GEOGRAFICOS                 \n",
      "7340                        CUESTIONES DE ORGANIZACION                 \n",
      "\n",
      "                                   label_fr alt_labels_fr  \\\n",
      "0                                GEOGRAPHIE                 \n",
      "1                              CARTOGRAPHIE                 \n",
      "2     FORMATIONS GEOLOGIQUES ET ECOSYSTEMES                 \n",
      "3                     RESSOURCES NATURELLES                 \n",
      "4                             ENVIRONNEMENT                 \n",
      "...                                     ...           ...   \n",
      "7336                                 EMPLOI                 \n",
      "7337                 ASSISTANCE HUMANITAIRE                 \n",
      "7338                 SCIENCES ET TECHNIQUES                 \n",
      "7339             DESCRIPTEURS GEOGRAPHIQUES                 \n",
      "7340           QUESTIONS ORGANISATIONNELLES                 \n",
      "\n",
      "                            label_ar alt_labels_ar  \\\n",
      "0                          الجغرافيا                 \n",
      "1                        رسم الخرائط                 \n",
      "2     أشكال الأرض والنظم الأيكولوجية                 \n",
      "3                       موارد (عامة)                 \n",
      "4                             البيئة                 \n",
      "...                              ...           ...   \n",
      "7336                         العمالة                 \n",
      "7337           معونات إنسانية وإغاثة                 \n",
      "7338              العلم والتكنولوجيا                 \n",
      "7339                  واصفات جغرافية                 \n",
      "7340                   مسائل تنظيمية                 \n",
      "\n",
      "                         label_ru alt_labels_ru     label_zh alt_labels_zh  \\\n",
      "0                       ГЕОГРАФИЯ                         地理                 \n",
      "1                     КАРТОГРАФИЯ                         制图                 \n",
      "2     ОЧЕРТАНИЯ СУШИ И ЭКОСИСТЕМЫ                    地形和生态系统                 \n",
      "3                 РЕСУРСЫ (ОБЩЕЕ)                资源 （一般意义上的）                 \n",
      "4                ОКРУЖАЮЩАЯ СРЕДА                         环境                 \n",
      "...                           ...           ...          ...           ...   \n",
      "7336                       РАБОТА                         就业                 \n",
      "7337          ГУМАНИТАРНАЯ ПОМОЩЬ                     人道主义救援                 \n",
      "7338           НАУКА И ТЕХНОЛОГИЯ                      科学和技术                 \n",
      "7339      ГЕОГРАФИЧЕСКИЕ ОПИСАНИЯ                       地理区划                 \n",
      "7340      ОРГАНИЗАЦИОННЫЕ ВОПРОСЫ                      组织内事务                 \n",
      "\n",
      "     node_type  \n",
      "0      concept  \n",
      "1      concept  \n",
      "2      concept  \n",
      "3      concept  \n",
      "4      concept  \n",
      "...        ...  \n",
      "7336    scheme  \n",
      "7337    scheme  \n",
      "7338    scheme  \n",
      "7339    scheme  \n",
      "7340    scheme  \n",
      "\n",
      "[7341 rows x 14 columns]}\n",
      "INFO - Parsing GA subjects using | and -- delimiters\n",
      "INFO - Expanded subjects from 443 to 255 unique subjects.\n",
      "INFO - Matching GA subjects to subject IDs\n",
      "INFO - Expanded subjects from 443 to 255 unique subjects.\n",
      "INFO - Matching GA subjects to subject IDs\n",
      "INFO - GA Subject Mapping Results:\n",
      "INFO -   Total rows: 5856\n",
      "INFO -   Matched: 3390 (57.9%)\n",
      "INFO -   Unmatched: 2466 (42.1%)\n",
      "INFO - Sample unmatched GA subjects:\n",
      "INFO -   - 'nan'\n",
      "INFO - GA Subject Mapping Results:\n",
      "INFO -   Total rows: 5856\n",
      "INFO -   Matched: 3390 (57.9%)\n",
      "INFO -   Unmatched: 2466 (42.1%)\n",
      "INFO - Sample unmatched GA subjects:\n",
      "INFO -   - 'nan'\n",
      "INFO -   - 'UN. ECONOMIC AND SOCIAL COUNCIL'\n",
      "INFO -   - 'UN. ECONOMIC AND SOCIAL COUNCIL'\n",
      "INFO -   - 'REVIEW CONFERENCE OF THE PARTIES TO THE TREATY ON THE NON-PROLIFERATION OF NUCLEAR WEAPONS (3RD : 1985 : GENEVA)'\n",
      "INFO -   - 'NUCLEAR WEAPON FREEZE'\n",
      "INFO -   - 'NEW INTERNATIONAL ECONOMIC ORDER'\n",
      "INFO - Processed 5856 GA resolutions with parsed subjects\n",
      "INFO -   - 'REVIEW CONFERENCE OF THE PARTIES TO THE TREATY ON THE NON-PROLIFERATION OF NUCLEAR WEAPONS (3RD : 1985 : GENEVA)'\n",
      "INFO -   - 'NUCLEAR WEAPON FREEZE'\n",
      "INFO -   - 'NEW INTERNATIONAL ECONOMIC ORDER'\n",
      "INFO - Processed 5856 GA resolutions with parsed subjects\n",
      "['undl_id', 'date', 'session', 'resolution', 'draft', 'committee_report', 'meeting', 'title', 'agenda_title', 'subjects', 'total_yes', 'total_no', 'total_abstentions', 'total_non_voting', 'total_ms', 'undl_link', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD', 'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BOL', 'BRA', 'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN', 'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CSK', 'CUB', 'CYP', 'CZE', 'DDR', 'DEU', 'DJI', 'DMA', 'DNK', 'DOM', 'DZA', 'EAT', 'EAZ', 'ECU', 'EGY', 'ERI', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM', 'GAB', 'GBR', 'GEO', 'GER', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC', 'GRD', 'GTM', 'GUY', 'HND', 'HRV', 'HTI', 'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA', 'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KIR', 'KNA', 'KOR', 'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LIE', 'LKA', 'LSO', 'LTU', 'LUX', 'LVA', 'MAR', 'MCO', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL', 'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MOZ', 'MRT', 'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR', 'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW', 'PNG', 'POL', 'PRK', 'PRT', 'PRY', 'QAT', 'ROU', 'RUS', 'RWA', 'SAU', 'SCG', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR', 'SOM', 'SRB', 'SSD', 'STP', 'SUN', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON', 'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA', 'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'YMD', 'YUG', 'ZAF', 'ZMB', 'ZWE', 'subject_id']\n",
      "INFO - \n",
      "Warning: 2432 resolutions have no mapped subjects\n",
      "['undl_id', 'date', 'session', 'resolution', 'draft', 'committee_report', 'meeting', 'title', 'agenda_title', 'subjects', 'total_yes', 'total_no', 'total_abstentions', 'total_non_voting', 'total_ms', 'undl_link', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD', 'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BOL', 'BRA', 'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN', 'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CSK', 'CUB', 'CYP', 'CZE', 'DDR', 'DEU', 'DJI', 'DMA', 'DNK', 'DOM', 'DZA', 'EAT', 'EAZ', 'ECU', 'EGY', 'ERI', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM', 'GAB', 'GBR', 'GEO', 'GER', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC', 'GRD', 'GTM', 'GUY', 'HND', 'HRV', 'HTI', 'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA', 'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KIR', 'KNA', 'KOR', 'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LIE', 'LKA', 'LSO', 'LTU', 'LUX', 'LVA', 'MAR', 'MCO', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL', 'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MOZ', 'MRT', 'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR', 'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW', 'PNG', 'POL', 'PRK', 'PRT', 'PRY', 'QAT', 'ROU', 'RUS', 'RWA', 'SAU', 'SCG', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR', 'SOM', 'SRB', 'SSD', 'STP', 'SUN', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON', 'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA', 'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'YMD', 'YUG', 'ZAF', 'ZMB', 'ZWE', 'subject_id']\n",
      "INFO - \n",
      "Warning: 2432 resolutions have no mapped subjects\n",
      "INFO - Initialization complete with fetched Data.\n",
      "INFO - Initialization complete with fetched Data.\n"
     ]
    }
   ],
   "source": [
    "datarep = DataRepository(config_path='config/data_sources.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e20bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Logging setup complete.\n",
      "INFO - Initializing UNDataRepository\n",
      "INFO - Initializing UNDataRepository\n",
      "INFO - URL is valid: https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Cached Data files not found.\n",
      "INFO - Fetching GA resolutions from https://digitallibrary.un.org/record/4060887/files/2025_9_19_ga_voting.csv\n",
      "INFO - URL is valid: https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Cached Data files not found.\n",
      "INFO - Fetching GA resolutions from https://digitallibrary.un.org/record/4060887/files/2025_9_19_ga_voting.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janic\\OneDrive\\Desktop\\ETH\\UN Projekt\\policy-pulse\\notebooks\\janic\\unDataStream\\fetchers\\ga_fetcher.py:24: DtypeWarning: Columns (5,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(source_config['url'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Successfully fetched 916554 GA resolution records\n",
      "INFO - Fetching thesaurus from https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Fetching thesaurus from https://digitallibrary.un.org/record/4075456/files/unbist-20250708_2.ttl\n",
      "INFO - Downloaded thesaurus file successfully\n",
      "INFO - Downloaded thesaurus file successfully\n",
      "INFO - Processing thesaurus data\n",
      "INFO - Processing thesaurus data\n",
      "INFO - Extracted 7341 subjects/schemes:\n",
      "INFO -   - Concepts: 7322\n",
      "INFO -   - Schemes: 19\n",
      "INFO - Extracted 7341 subjects/schemes:\n",
      "INFO -   - Concepts: 7322\n",
      "INFO -   - Schemes: 19\n",
      "INFO - Found 7341 subjects/schemes\n",
      "INFO - Found 12776 parent-child relationships\n",
      "INFO - Found 7341 subjects/schemes\n",
      "INFO - Found 12776 parent-child relationships\n",
      "INFO - Closure table statistics:\n",
      "INFO -   Total rows: 39,026\n",
      "INFO -   Nodes with ancestors: 7,322\n",
      "INFO -   Unique ancestors: 7,341\n",
      "INFO - Closure table statistics:\n",
      "INFO -   Total rows: 39,026\n",
      "INFO -   Nodes with ancestors: 7,322\n",
      "INFO -   Unique ancestors: 7,341\n",
      "INFO -   Unique descendants: 7,341\n",
      "INFO -   Max depth: 7\n",
      "INFO - Depth distribution:\n",
      "INFO -   Depth 0: 7,341 relationships\n",
      "INFO -   Depth 1: 12,776 relationships\n",
      "INFO -   Depth 2: 12,609 relationships\n",
      "INFO -   Depth 3: 3,878 relationships\n",
      "INFO -   Depth 4: 1,787 relationships\n",
      "INFO -   Depth 5: 552 relationships\n",
      "INFO -   Depth 6: 75 relationships\n",
      "INFO -   Depth 7: 8 relationships\n",
      "INFO - Processing GA resolution data\n",
      "INFO -   Unique descendants: 7,341\n",
      "INFO -   Max depth: 7\n",
      "INFO - Depth distribution:\n",
      "INFO -   Depth 0: 7,341 relationships\n",
      "INFO -   Depth 1: 12,776 relationships\n",
      "INFO -   Depth 2: 12,609 relationships\n",
      "INFO -   Depth 3: 3,878 relationships\n",
      "INFO -   Depth 4: 1,787 relationships\n",
      "INFO -   Depth 5: 552 relationships\n",
      "INFO -   Depth 6: 75 relationships\n",
      "INFO -   Depth 7: 8 relationships\n",
      "INFO - Processing GA resolution data\n",
      "INFO - Transformed 5534 GA resolutions\n",
      "INFO - Parsing GA subjects using | and -- delimiters\n",
      "INFO - Transformed 5534 GA resolutions\n",
      "INFO - Parsing GA subjects using | and -- delimiters\n",
      "INFO - Expanded subjects from 443 to 255 unique subjects.\n",
      "INFO - Matching GA subjects to subject IDs\n",
      "INFO - Expanded subjects from 443 to 255 unique subjects.\n",
      "INFO - Matching GA subjects to subject IDs\n",
      "INFO - GA Subject Mapping Results:\n",
      "INFO -   Total rows: 5856\n",
      "INFO -   Matched: 3390 (57.9%)\n",
      "INFO -   Unmatched: 2466 (42.1%)\n",
      "INFO - Sample unmatched GA subjects:\n",
      "INFO -   - 'nan'\n",
      "INFO -   - 'UN. ECONOMIC AND SOCIAL COUNCIL'\n",
      "INFO - GA Subject Mapping Results:\n",
      "INFO -   Total rows: 5856\n",
      "INFO -   Matched: 3390 (57.9%)\n",
      "INFO -   Unmatched: 2466 (42.1%)\n",
      "INFO - Sample unmatched GA subjects:\n",
      "INFO -   - 'nan'\n",
      "INFO -   - 'UN. ECONOMIC AND SOCIAL COUNCIL'\n",
      "INFO -   - 'REVIEW CONFERENCE OF THE PARTIES TO THE TREATY ON THE NON-PROLIFERATION OF NUCLEAR WEAPONS (3RD : 1985 : GENEVA)'\n",
      "INFO -   - 'NUCLEAR WEAPON FREEZE'\n",
      "INFO -   - 'NEW INTERNATIONAL ECONOMIC ORDER'\n",
      "INFO - Processed 5856 GA resolutions with parsed subjects\n",
      "INFO -   - 'REVIEW CONFERENCE OF THE PARTIES TO THE TREATY ON THE NON-PROLIFERATION OF NUCLEAR WEAPONS (3RD : 1985 : GENEVA)'\n",
      "INFO -   - 'NUCLEAR WEAPON FREEZE'\n",
      "INFO -   - 'NEW INTERNATIONAL ECONOMIC ORDER'\n",
      "INFO - Processed 5856 GA resolutions with parsed subjects\n",
      "INFO - \n",
      "Warning: 2432 resolutions have no mapped subjects\n",
      "INFO - \n",
      "Warning: 2432 resolutions have no mapped subjects\n",
      "INFO - Initialization complete with fetched Data.\n",
      "INFO - Initialization complete with fetched Data.\n"
     ]
    }
   ],
   "source": [
    "from unDataStream import DataRepository\n",
    "\n",
    "repository = DataRepository(config_path='config/data_sources.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70a5a8",
   "metadata": {},
   "source": [
    "# File Comparison Utility\n",
    "\n",
    "This function compares files between two directories to identify differences - useful for testing data processing outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0edd0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import filecmp\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import difflib\n",
    "\n",
    "def compare_directories(dir1: str, dir2: str, \n",
    "                       file_extensions: Optional[List[str]] = None,\n",
    "                       detailed_diff: bool = True,\n",
    "                       ignore_whitespace: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare files between two directories and report differences.\n",
    "    \n",
    "    Args:\n",
    "        dir1 (str): Path to first directory\n",
    "        dir2 (str): Path to second directory\n",
    "        file_extensions (List[str], optional): Only compare files with these extensions \n",
    "                                              (e.g., ['.csv', '.txt']). If None, compare all files.\n",
    "        detailed_diff (bool): If True, show detailed line-by-line differences for text files\n",
    "        ignore_whitespace (bool): If True, ignore whitespace differences in text comparisons\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results with the following structure:\n",
    "            {\n",
    "                'summary': {\n",
    "                    'total_files_dir1': int,\n",
    "                    'total_files_dir2': int,\n",
    "                    'identical_files': int,\n",
    "                    'different_files': int,\n",
    "                    'only_in_dir1': int,\n",
    "                    'only_in_dir2': int\n",
    "                },\n",
    "                'identical': List[str],           # Files that are identical\n",
    "                'different': List[str],           # Files that differ\n",
    "                'only_in_dir1': List[str],        # Files only in directory 1\n",
    "                'only_in_dir2': List[str],        # Files only in directory 2\n",
    "                'differences': Dict[str, str]     # Detailed differences for each different file\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    dir1_path = Path(dir1)\n",
    "    dir2_path = Path(dir2)\n",
    "    \n",
    "    # Validate directories exist\n",
    "    if not dir1_path.exists():\n",
    "        raise ValueError(f\"Directory 1 does not exist: {dir1}\")\n",
    "    if not dir2_path.exists():\n",
    "        raise ValueError(f\"Directory 2 does not exist: {dir2}\")\n",
    "    \n",
    "    # Get all files in both directories\n",
    "    def get_files(directory: Path, extensions: Optional[List[str]] = None) -> Dict[str, Path]:\n",
    "        \"\"\"Get all files in directory, optionally filtered by extension.\"\"\"\n",
    "        files = {}\n",
    "        for file_path in directory.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                # Filter by extension if specified\n",
    "                if extensions is None or file_path.suffix.lower() in [ext.lower() for ext in extensions]:\n",
    "                    # Use relative path as key for comparison\n",
    "                    rel_path = file_path.relative_to(directory)\n",
    "                    files[str(rel_path)] = file_path\n",
    "        return files\n",
    "    \n",
    "    files1 = get_files(dir1_path, file_extensions)\n",
    "    files2 = get_files(dir2_path, file_extensions)\n",
    "    \n",
    "    # Find common files and unique files\n",
    "    common_files = set(files1.keys()) & set(files2.keys())\n",
    "    only_in_dir1 = set(files1.keys()) - set(files2.keys())\n",
    "    only_in_dir2 = set(files2.keys()) - set(files1.keys())\n",
    "    \n",
    "    # Compare common files\n",
    "    identical = []\n",
    "    different = []\n",
    "    differences = {}\n",
    "    \n",
    "    for filename in common_files:\n",
    "        file1_path = files1[filename]\n",
    "        file2_path = files2[filename]\n",
    "        \n",
    "        if _files_are_identical(file1_path, file2_path, ignore_whitespace):\n",
    "            identical.append(filename)\n",
    "        else:\n",
    "            different.append(filename)\n",
    "            \n",
    "            # Get detailed differences if requested\n",
    "            if detailed_diff:\n",
    "                diff_text = _get_file_differences(file1_path, file2_path, ignore_whitespace)\n",
    "                differences[filename] = diff_text\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        'total_files_dir1': len(files1),\n",
    "        'total_files_dir2': len(files2),\n",
    "        'identical_files': len(identical),\n",
    "        'different_files': len(different),\n",
    "        'only_in_dir1': len(only_in_dir1),\n",
    "        'only_in_dir2': len(only_in_dir2)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'identical': sorted(identical),\n",
    "        'different': sorted(different),\n",
    "        'only_in_dir1': sorted(only_in_dir1),\n",
    "        'only_in_dir2': sorted(only_in_dir2),\n",
    "        'differences': differences\n",
    "    }\n",
    "\n",
    "def _files_are_identical(file1: Path, file2: Path, ignore_whitespace: bool = False) -> bool:\n",
    "    \"\"\"Check if two files are identical.\"\"\"\n",
    "    \n",
    "    # First check file sizes\n",
    "    if file1.stat().st_size != file2.stat().st_size and not ignore_whitespace:\n",
    "        return False\n",
    "    \n",
    "    # For CSV files, use pandas comparison if possible\n",
    "    if file1.suffix.lower() == '.csv' and file2.suffix.lower() == '.csv':\n",
    "        try:\n",
    "            df1 = pd.read_csv(file1)\n",
    "            df2 = pd.read_csv(file2)\n",
    "            return df1.equals(df2)\n",
    "        except Exception:\n",
    "            # Fall back to text comparison if pandas fails\n",
    "            pass\n",
    "    \n",
    "    # For text files, compare content\n",
    "    try:\n",
    "        with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "            content1 = f1.read()\n",
    "            content2 = f2.read()\n",
    "            \n",
    "            if ignore_whitespace:\n",
    "                # Remove all whitespace for comparison\n",
    "                content1 = ''.join(content1.split())\n",
    "                content2 = ''.join(content2.split())\n",
    "            \n",
    "            return content1 == content2\n",
    "    except UnicodeDecodeError:\n",
    "        # Binary comparison for non-text files\n",
    "        return _binary_files_identical(file1, file2)\n",
    "\n",
    "def _binary_files_identical(file1: Path, file2: Path) -> bool:\n",
    "    \"\"\"Compare binary files using hash.\"\"\"\n",
    "    def file_hash(filepath):\n",
    "        hasher = hashlib.md5()\n",
    "        with open(filepath, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hasher.update(chunk)\n",
    "        return hasher.hexdigest()\n",
    "    \n",
    "    return file_hash(file1) == file_hash(file2)\n",
    "\n",
    "def _get_file_differences(file1: Path, file2: Path, ignore_whitespace: bool = False) -> str:\n",
    "    \"\"\"Get detailed differences between two files.\"\"\"\n",
    "    \n",
    "    # For CSV files, provide structured comparison\n",
    "    if file1.suffix.lower() == '.csv' and file2.suffix.lower() == '.csv':\n",
    "        try:\n",
    "            df1 = pd.read_csv(file1)\n",
    "            df2 = pd.read_csv(file2)\n",
    "            \n",
    "            differences = []\n",
    "            \n",
    "            # Check shapes\n",
    "            if df1.shape != df2.shape:\n",
    "                differences.append(f\"Shape difference: {df1.shape} vs {df2.shape}\")\n",
    "            \n",
    "            # Check columns\n",
    "            if list(df1.columns) != list(df2.columns):\n",
    "                differences.append(f\"Column difference:\")\n",
    "                differences.append(f\"  File1 columns: {list(df1.columns)}\")\n",
    "                differences.append(f\"  File2 columns: {list(df2.columns)}\")\n",
    "            \n",
    "            # Check data differences (sample)\n",
    "            if df1.shape == df2.shape and list(df1.columns) == list(df2.columns):\n",
    "                # Compare values\n",
    "                comparison = df1 != df2\n",
    "                if comparison.any().any():\n",
    "                    differences.append(\"Data differences found:\")\n",
    "                    # Show first few different rows\n",
    "                    diff_rows = comparison.any(axis=1)\n",
    "                    sample_diffs = df1[diff_rows].head(3)\n",
    "                    differences.append(f\"Sample different rows in file1:\\n{sample_diffs}\")\n",
    "                    \n",
    "                    sample_diffs2 = df2[diff_rows].head(3)\n",
    "                    differences.append(f\"Sample different rows in file2:\\n{sample_diffs2}\")\n",
    "            \n",
    "            return '\\n'.join(differences)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error comparing CSV files: {e}\"\n",
    "    \n",
    "    # For text files, show line-by-line differences\n",
    "    try:\n",
    "        with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "            lines1 = f1.readlines()\n",
    "            lines2 = f2.readlines()\n",
    "            \n",
    "            if ignore_whitespace:\n",
    "                lines1 = [line.strip() for line in lines1]\n",
    "                lines2 = [line.strip() for line in lines2]\n",
    "            \n",
    "            # Generate unified diff\n",
    "            diff = difflib.unified_diff(\n",
    "                lines1, lines2,\n",
    "                fromfile=str(file1),\n",
    "                tofile=str(file2),\n",
    "                lineterm=''\n",
    "            )\n",
    "            \n",
    "            diff_text = '\\n'.join(list(diff)[:100])  # Limit to first 100 lines\n",
    "            if len(list(difflib.unified_diff(lines1, lines2))) > 100:\n",
    "                diff_text += \"\\n... (truncated, too many differences)\"\n",
    "            \n",
    "            return diff_text\n",
    "            \n",
    "    except UnicodeDecodeError:\n",
    "        return \"Binary files differ (cannot show text diff)\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating diff: {e}\"\n",
    "\n",
    "def print_comparison_report(comparison_result: Dict, show_details: bool = True):\n",
    "    \"\"\"Print a formatted report of the comparison results.\"\"\"\n",
    "    \n",
    "    result = comparison_result\n",
    "    summary = result['summary']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DIRECTORY COMPARISON REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"  Files in directory 1: {summary['total_files_dir1']}\")\n",
    "    print(f\"  Files in directory 2: {summary['total_files_dir2']}\")\n",
    "    print(f\"  Identical files: {summary['identical_files']}\")\n",
    "    print(f\"  Different files: {summary['different_files']}\")\n",
    "    print(f\"  Only in dir1: {summary['only_in_dir1']}\")\n",
    "    print(f\"  Only in dir2: {summary['only_in_dir2']}\")\n",
    "    \n",
    "    # Calculate success percentage\n",
    "    total_common = summary['identical_files'] + summary['different_files']\n",
    "    if total_common > 0:\n",
    "        success_rate = (summary['identical_files'] / total_common) * 100\n",
    "        print(f\"  Match rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Show identical files\n",
    "    if result['identical']:\n",
    "        print(f\"\\n✅ IDENTICAL FILES ({len(result['identical'])}):\")\n",
    "        for filename in result['identical'][:10]:  # Show first 10\n",
    "            print(f\"  • {filename}\")\n",
    "        if len(result['identical']) > 10:\n",
    "            print(f\"  ... and {len(result['identical']) - 10} more\")\n",
    "    \n",
    "    # Show different files\n",
    "    if result['different']:\n",
    "        print(f\"\\n❌ DIFFERENT FILES ({len(result['different'])}):\")\n",
    "        for filename in result['different']:\n",
    "            print(f\"  • {filename}\")\n",
    "            if show_details and filename in result['differences']:\n",
    "                # Show first few lines of diff\n",
    "                diff_lines = result['differences'][filename].split('\\n')[:5]\n",
    "                for line in diff_lines:\n",
    "                    if line.strip():\n",
    "                        print(f\"    {line}\")\n",
    "                if len(result['differences'][filename].split('\\n')) > 5:\n",
    "                    print(\"    ... (more differences)\")\n",
    "                print()\n",
    "    \n",
    "    # Show files only in dir1\n",
    "    if result['only_in_dir1']:\n",
    "        print(f\"\\n📁 ONLY IN DIRECTORY 1 ({len(result['only_in_dir1'])}):\")\n",
    "        for filename in result['only_in_dir1'][:10]:\n",
    "            print(f\"  • {filename}\")\n",
    "        if len(result['only_in_dir1']) > 10:\n",
    "            print(f\"  ... and {len(result['only_in_dir1']) - 10} more\")\n",
    "    \n",
    "    # Show files only in dir2\n",
    "    if result['only_in_dir2']:\n",
    "        print(f\"\\n📁 ONLY IN DIRECTORY 2 ({len(result['only_in_dir2'])}):\")\n",
    "        for filename in result['only_in_dir2'][:10]:\n",
    "            print(f\"  • {filename}\")\n",
    "        if len(result['only_in_dir2']) > 10:\n",
    "            print(f\"  ... and {len(result['only_in_dir2']) - 10} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65619f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing directory comparison function...\n",
      "============================================================\n",
      "DIRECTORY COMPARISON REPORT\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Files in directory 1: 5\n",
      "  Files in directory 2: 5\n",
      "  Identical files: 2\n",
      "  Different files: 2\n",
      "  Only in dir1: 1\n",
      "  Only in dir2: 1\n",
      "  Match rate: 50.0%\n",
      "\n",
      "✅ IDENTICAL FILES (2):\n",
      "  • identical.txt\n",
      "  • identical_data.csv\n",
      "\n",
      "❌ DIFFERENT FILES (2):\n",
      "  • different.txt\n",
      "    --- C:\\Users\\janic\\AppData\\Local\\Temp\\tmpa4kzvof5\\dir1\\different.txt\n",
      "    +++ C:\\Users\\janic\\AppData\\Local\\Temp\\tmpa4kzvof5\\dir2\\different.txt\n",
      "    @@ -1,2 +1,2 @@\n",
      "    -This is different content in dir1\n",
      "    ... (more differences)\n",
      "\n",
      "  • test_data.csv\n",
      "    Data differences found:\n",
      "    Sample different rows in file1:\n",
      "       A  B\n",
      "    2  3  z\n",
      "    Sample different rows in file2:\n",
      "    ... (more differences)\n",
      "\n",
      "\n",
      "📁 ONLY IN DIRECTORY 1 (1):\n",
      "  • only_in_dir1.txt\n",
      "\n",
      "📁 ONLY IN DIRECTORY 2 (1):\n",
      "  • only_in_dir2.txt\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the directory comparison function\n",
    "\n",
    "# Example 1: Compare two data directories\n",
    "def test_directory_comparison():\n",
    "    \"\"\"Example of how to use the directory comparison function.\"\"\"\n",
    "    \n",
    "    # Create some test directories and files for demonstration\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    # Create temporary directories\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        dir1 = os.path.join(temp_dir, \"dir1\")\n",
    "        dir2 = os.path.join(temp_dir, \"dir2\") \n",
    "        \n",
    "        os.makedirs(dir1, exist_ok=True)\n",
    "        os.makedirs(dir2, exist_ok=True)\n",
    "        \n",
    "        # Create some test files\n",
    "        # Identical file\n",
    "        with open(os.path.join(dir1, \"identical.txt\"), \"w\") as f:\n",
    "            f.write(\"This is the same content\\nLine 2\\nLine 3\")\n",
    "        with open(os.path.join(dir2, \"identical.txt\"), \"w\") as f:\n",
    "            f.write(\"This is the same content\\nLine 2\\nLine 3\")\n",
    "        \n",
    "        # Different file\n",
    "        with open(os.path.join(dir1, \"different.txt\"), \"w\") as f:\n",
    "            f.write(\"This is different content in dir1\\nLine 2\")\n",
    "        with open(os.path.join(dir2, \"different.txt\"), \"w\") as f:\n",
    "            f.write(\"This is different content in dir2\\nLine 2 changed\")\n",
    "        \n",
    "        # File only in dir1\n",
    "        with open(os.path.join(dir1, \"only_in_dir1.txt\"), \"w\") as f:\n",
    "            f.write(\"This file is only in directory 1\")\n",
    "        \n",
    "        # File only in dir2\n",
    "        with open(os.path.join(dir2, \"only_in_dir2.txt\"), \"w\") as f:\n",
    "            f.write(\"This file is only in directory 2\")\n",
    "        \n",
    "        # CSV files for testing\n",
    "        import pandas as pd\n",
    "        df1 = pd.DataFrame({'A': [1, 2, 3], 'B': ['x', 'y', 'z']})\n",
    "        df2 = pd.DataFrame({'A': [1, 2, 4], 'B': ['x', 'y', 'z']})  # Different data\n",
    "        df3 = pd.DataFrame({'A': [1, 2, 3], 'B': ['x', 'y', 'z']})  # Identical\n",
    "        \n",
    "        df1.to_csv(os.path.join(dir1, \"test_data.csv\"), index=False)\n",
    "        df2.to_csv(os.path.join(dir2, \"test_data.csv\"), index=False)\n",
    "        \n",
    "        df3.to_csv(os.path.join(dir1, \"identical_data.csv\"), index=False)\n",
    "        df3.to_csv(os.path.join(dir2, \"identical_data.csv\"), index=False)\n",
    "        \n",
    "        # Run comparison\n",
    "        print(\"🧪 Testing directory comparison function...\")\n",
    "        comparison = compare_directories(dir1, dir2, detailed_diff=True)\n",
    "        \n",
    "        # Print results\n",
    "        print_comparison_report(comparison, show_details=True)\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "# Run the test\n",
    "test_result = test_directory_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca06096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Directory comparison functions are ready!\n",
      "\n",
      "Usage examples:\n",
      "1. compare_directories(dir1, dir2) - Compare any two directories\n",
      "2. compare_pipeline_outputs(expected_dir, actual_dir) - Compare pipeline outputs\n",
      "3. Add file_extensions=['.csv'] to compare only CSV files\n",
      "4. Add ignore_whitespace=True to ignore whitespace differences\n"
     ]
    }
   ],
   "source": [
    "# Practical examples for your UN data pipeline testing\n",
    "\n",
    "def compare_pipeline_outputs(expected_dir: str, actual_dir: str):\n",
    "    \"\"\"\n",
    "    Compare expected vs actual outputs from your UN data pipeline.\n",
    "    \n",
    "    This is specifically designed for testing your data processing pipeline\n",
    "    by comparing the generated CSV files with expected results.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 Comparing UN Data Pipeline Outputs...\")\n",
    "    print(f\"Expected results: {expected_dir}\")\n",
    "    print(f\"Actual results: {actual_dir}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compare only CSV files (your main output format)\n",
    "    comparison = compare_directories(\n",
    "        expected_dir, \n",
    "        actual_dir,\n",
    "        file_extensions=['.csv'],\n",
    "        detailed_diff=True,\n",
    "        ignore_whitespace=False\n",
    "    )\n",
    "    \n",
    "    # Print detailed report\n",
    "    print_comparison_report(comparison, show_details=True)\n",
    "    \n",
    "    # Additional analysis for CSV files\n",
    "    if comparison['different']:\n",
    "        print(\"\\n📊 DETAILED CSV ANALYSIS:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for filename in comparison['different']:\n",
    "            if filename.endswith('.csv'):\n",
    "                print(f\"\\n🔍 Analyzing {filename}:\")\n",
    "                \n",
    "                expected_file = Path(expected_dir) / filename\n",
    "                actual_file = Path(actual_dir) / filename\n",
    "                \n",
    "                try:\n",
    "                    df_expected = pd.read_csv(expected_file)\n",
    "                    df_actual = pd.read_csv(actual_file)\n",
    "                    \n",
    "                    print(f\"  Expected shape: {df_expected.shape}\")\n",
    "                    print(f\"  Actual shape: {df_actual.shape}\")\n",
    "                    \n",
    "                    if list(df_expected.columns) != list(df_actual.columns):\n",
    "                        print(f\"  ❌ Column mismatch!\")\n",
    "                        print(f\"    Expected: {list(df_expected.columns)}\")\n",
    "                        print(f\"    Actual: {list(df_actual.columns)}\")\n",
    "                    else:\n",
    "                        print(f\"  ✅ Columns match\")\n",
    "                        \n",
    "                        # Check for data differences in common columns\n",
    "                        if df_expected.shape == df_actual.shape:\n",
    "                            differences = (df_expected != df_actual).sum().sum()\n",
    "                            total_cells = df_expected.shape[0] * df_expected.shape[1]\n",
    "                            print(f\"  Data differences: {differences}/{total_cells} cells\")\n",
    "                            \n",
    "                            if differences > 0:\n",
    "                                # Show which columns have differences\n",
    "                                col_diffs = (df_expected != df_actual).sum()\n",
    "                                problem_cols = col_diffs[col_diffs > 0]\n",
    "                                print(f\"  Columns with differences: {dict(problem_cols)}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ❌ Error analyzing CSV: {e}\")\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Example usage for your specific case:\n",
    "# comparison = compare_pipeline_outputs(\"expected_output/\", \"data/\")\n",
    "\n",
    "print(\"✅ Directory comparison functions are ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"1. compare_directories(dir1, dir2) - Compare any two directories\")\n",
    "print(\"2. compare_pipeline_outputs(expected_dir, actual_dir) - Compare pipeline outputs\")\n",
    "print(\"3. Add file_extensions=['.csv'] to compare only CSV files\")\n",
    "print(\"4. Add ignore_whitespace=True to ignore whitespace differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec5c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Comparing UN Data Pipeline Outputs...\n",
      "Expected results: data/\n",
      "Actual results: data1/\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janic\\AppData\\Local\\Temp\\ipykernel_18336\\1403192892.py:121: DtypeWarning: Columns (67,68,169,179) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1)\n",
      "C:\\Users\\janic\\AppData\\Local\\Temp\\ipykernel_18336\\1403192892.py:122: DtypeWarning: Columns (67,68,169,179) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(file2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIRECTORY COMPARISON REPORT\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Files in directory 1: 4\n",
      "  Files in directory 2: 4\n",
      "  Identical files: 3\n",
      "  Different files: 1\n",
      "  Only in dir1: 0\n",
      "  Only in dir2: 0\n",
      "  Match rate: 75.0%\n",
      "\n",
      "✅ IDENTICAL FILES (3):\n",
      "  • resolution_subject_table.csv\n",
      "  • resolution_table.csv\n",
      "  • subject_table.csv\n",
      "\n",
      "❌ DIFFERENT FILES (1):\n",
      "  • closure_table.csv\n",
      "    Data differences found:\n",
      "    Sample different rows in file1:\n",
      "                                    ancestor_id  \\\n",
      "    0  http://metadata.un.org/thesaurus/1001577   \n",
      "    1  http://metadata.un.org/thesaurus/1005673   \n",
      "    ... (more differences)\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "📊 DETAILED CSV ANALYSIS:\n",
      "------------------------------\n",
      "\n",
      "🔍 Analyzing closure_table.csv:\n",
      "  Expected shape: (39026, 3)\n",
      "  Actual shape: (39026, 3)\n",
      "  ✅ Columns match\n",
      "  Data differences: 106570/117078 cells\n",
      "  Columns with differences: {'ancestor_id': np.int64(38768), 'descendant_id': np.int64(39026), 'depth': np.int64(28776)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'total_files_dir1': 4,\n",
       "  'total_files_dir2': 4,\n",
       "  'identical_files': 3,\n",
       "  'different_files': 1,\n",
       "  'only_in_dir1': 0,\n",
       "  'only_in_dir2': 0},\n",
       " 'identical': ['resolution_subject_table.csv',\n",
       "  'resolution_table.csv',\n",
       "  'subject_table.csv'],\n",
       " 'different': ['closure_table.csv'],\n",
       " 'only_in_dir1': [],\n",
       " 'only_in_dir2': [],\n",
       " 'differences': {'closure_table.csv': 'Data differences found:\\nSample different rows in file1:\\n                                ancestor_id  \\\\\\n0  http://metadata.un.org/thesaurus/1001577   \\n1  http://metadata.un.org/thesaurus/1005673   \\n2   http://metadata.un.org/thesaurus/180100   \\n\\n                              descendant_id  depth  \\n0  http://metadata.un.org/thesaurus/1001577      0  \\n1  http://metadata.un.org/thesaurus/1001577      1  \\n2  http://metadata.un.org/thesaurus/1001577      1  \\nSample different rows in file2:\\n                                ancestor_id  \\\\\\n0  http://metadata.un.org/thesaurus/1004826   \\n1   http://metadata.un.org/thesaurus/140502   \\n2       http://metadata.un.org/thesaurus/14   \\n\\n                              descendant_id  depth  \\n0  http://metadata.un.org/thesaurus/1004826      0  \\n1  http://metadata.un.org/thesaurus/1004826      1  \\n2  http://metadata.un.org/thesaurus/1004826      2  '}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_pipeline_outputs('data/', 'data1/')  # Adjust paths as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2113b0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>descendant_id</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19498</th>\n",
       "      <td>http://metadata.un.org/thesaurus/00</td>\n",
       "      <td>http://metadata.un.org/thesaurus/00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>http://metadata.un.org/thesaurus/01</td>\n",
       "      <td>http://metadata.un.org/thesaurus/01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16643</th>\n",
       "      <td>http://metadata.un.org/thesaurus/01</td>\n",
       "      <td>http://metadata.un.org/thesaurus/010100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ancestor_id  \\\n",
       "19498  http://metadata.un.org/thesaurus/00   \n",
       "4005   http://metadata.un.org/thesaurus/01   \n",
       "16643  http://metadata.un.org/thesaurus/01   \n",
       "\n",
       "                                 descendant_id  depth  \n",
       "19498      http://metadata.un.org/thesaurus/00      0  \n",
       "4005       http://metadata.un.org/thesaurus/01      0  \n",
       "16643  http://metadata.un.org/thesaurus/010100      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closure_table1 = pd.read_csv('data/closure_table.csv')\n",
    "closure_table2 = pd.read_csv('data1/closure_table.csv')\n",
    "\n",
    "closure_table1.sort_values(['ancestor_id', 'descendant_id'], inplace=True)\n",
    "closure_table2.sort_values(['ancestor_id', 'descendant_id'], inplace=True)\n",
    "\n",
    "closure_table1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee7d853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>descendant_id</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>http://metadata.un.org/thesaurus/00</td>\n",
       "      <td>http://metadata.un.org/thesaurus/00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25123</th>\n",
       "      <td>http://metadata.un.org/thesaurus/01</td>\n",
       "      <td>http://metadata.un.org/thesaurus/01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>http://metadata.un.org/thesaurus/01</td>\n",
       "      <td>http://metadata.un.org/thesaurus/010100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ancestor_id  \\\n",
       "3641   http://metadata.un.org/thesaurus/00   \n",
       "25123  http://metadata.un.org/thesaurus/01   \n",
       "8256   http://metadata.un.org/thesaurus/01   \n",
       "\n",
       "                                 descendant_id  depth  \n",
       "3641       http://metadata.un.org/thesaurus/00      0  \n",
       "25123      http://metadata.un.org/thesaurus/01      0  \n",
       "8256   http://metadata.un.org/thesaurus/010100      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closure_table2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d53da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un-projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
